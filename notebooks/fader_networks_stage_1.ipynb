{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Thu 28-Jun to help look through the initial stages of developing the fader networks architecture.\n",
    "\n",
    "Fri 29-Jun: Haven't really used this today. Still all rather messy unfortunately/\n",
    "\n",
    "Runs in '/home/ubuntu/msc-project-fader-networks'\n",
    "on aws-big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/msc-project-fader-networks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import visdom\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data import NpzFolder, NpzLoader, TBPTTIter\n",
    "from model import Loop, MaskedMSE\n",
    "from utils import create_output_dir, wrap, check_grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import notebook_utils as nu\n",
    "\n",
    "import speaker_recognition as sr\n",
    "\n",
    "import evaluate_loss_func_for_notebook as el\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import sklearn.metrics.pairwise as pw\n",
    "\n",
    "from IPython.display import Audio\n",
    "import IPython.display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the VCTK dataset\n",
    "vctk_folder = '/home/ubuntu/VCTK-Corpus/'\n",
    "\n",
    "# location of the raw pre-calculated feature files for VCTK from Jiameng\n",
    "vctk_prebuilt_raw_folder = '/home/ubuntu/vctk-16khz-cmu-no-boundaries/'\n",
    "\n",
    "# location of the float32, train/validation files for VCTK-all\n",
    "vctk_precalc_folder = '/home/ubuntu/loop/data/vctk-16khz-cmu-no-boundaries-all'\n",
    "\n",
    "#vctk_raw_folder = '/home/ubuntu/VCTK-Corpus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "seed = 1\n",
    "data = '/home/ubuntu/loop/data/vctk'\n",
    "nspk = 22\n",
    "max_seq_len = 1000\n",
    "seq_len = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(gpu)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = 'models/vctk/bestmodel.pth'\n",
    "\n",
    "# model checkpoint\n",
    "checkpoint_file = '/home/ubuntu/loop/checkpoints/vctk-16khz-cmu-no-boundaries-all-noise-2/bestmodel.pth'\n",
    "#checkpoint_file = 'checkpoints/vctk-16khz-cmu-no-boundaries-all/lastmodel.pth'\n",
    "\n",
    "# data for this model\n",
    "data='/home/ubuntu/loop/data/vctk-16khz-cmu-no-boundaries-all'\n",
    "\n",
    "# WORLD feature normalisation data\n",
    "norm_path = '/home/ubuntu/loop/data/vctk-16khz-cmu-no-boundaries-all/norm_info/norm.dat'\n",
    "\n",
    "# output embeddings to a file\n",
    "output_file = '/tmp/embedding_file.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_info = nu.get_vctk_speaker_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Surrey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>227</td>\n",
       "      <td>38</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Cumbria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age gender  accents            region\n",
       "0  225   23      F  English  Southern England\n",
       "1  226   22      M  English            Surrey\n",
       "2  227   38      M  English           Cumbria\n",
       "3  228   22      F  English  Southern England\n",
       "4  229   23      F  English  Southern England"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = el.get_loader(data_path = vctk_precalc_folder)\n",
    "\n",
    "loop_speaker_lookup = loader.dataset.speakers # dict['p330'] = 88\n",
    "\n",
    "# Create dict from IDs used inside VoiceLoop to VCTK speaker IDs\n",
    "speaker_list_vctk = [int(k[1:])for k in loop_speaker_lookup.keys()] # list of VCTK speaker IDs; strip out the 'p'\n",
    "speaker_dict_vctk_to_loop = dict(zip(speaker_list_vctk, loop_speaker_lookup.values())) # dict['vctk_id] = sim_id\n",
    "speaker_dict_loop_to_vctk = dict(zip(loop_speaker_lookup.values(), speaker_list_vctk)) # dict['sim_id] = vctk_id\n",
    "\n",
    "# get dataframe for VCTK reference data indexed by VoiceLoop speaker ID\n",
    "tmp = pd.DataFrame.from_dict(speaker_dict_loop_to_vctk, orient='index', columns=['id'])\n",
    "speaker_info_loop = pd.merge(speaker_info, tmp)\n",
    "\n",
    "# examples\n",
    "speaker_info_loop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_args_path = os.path.join(os.path.dirname(checkpoint_file), 'args.pth')\n",
    "checkpoint_args = torch.load(checkpoint_args_path)\n",
    "\n",
    "# restore the model from the checkpoint\n",
    "checkpoint = torch.load(checkpoint_file,\n",
    "                     map_location=lambda storage, loc: storage)\n",
    "\n",
    "model = Loop(checkpoint_args[0])\n",
    "\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# extract the speaker embeddings from the model\n",
    "embeddings = model.encoder.lut_s.weight.data.numpy()\n",
    "\n",
    "# save the embeddings\n",
    "np.save(output_file, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(K=10, attention_alignment=0.05, batch_size=32, checkpoint='checkpoints/vctk-16khz-cmu-no-boundaries-all/bestmodel.pth', clip_grad=0.5, data='data/vctk-16khz-cmu-no-boundaries-all', epochs=90, expName='checkpoints/vctk-16khz-cmu-no-boundaries-all-noise-2', gpu=0, hidden_size=256, hidden_size_speakers=256, ignore_grad=10000.0, lr=0.0001, max_seq_len=1000, mem_size=20, noise=2, nspk=108, output_size=63, seed=1, seq_len=500, visualize=False, vocabulary_size=44)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view parameters used the train the model\n",
    "checkpoint_args[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loop(\n",
       "  (encoder): Encoder(\n",
       "    (lut_p): Embedding(44, 256, max_norm=1.0)\n",
       "    (lut_s): Embedding(108, 256, max_norm=1.0)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attn): GravesAttention(\n",
       "      (sm): Softmax()\n",
       "      (N_a): Sequential(\n",
       "        (0): Linear(in_features=6380, out_features=638, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=638, out_features=30, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (N_o): Sequential(\n",
       "      (0): Linear(in_features=6380, out_features=638, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=638, out_features=256, bias=True)\n",
       "    )\n",
       "    (output): Linear(in_features=256, out_features=63, bias=True)\n",
       "    (N_u): Sequential(\n",
       "      (0): Linear(in_features=6380, out_features=638, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=638, out_features=319, bias=True)\n",
       "    )\n",
       "    (F_u): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (F_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view model structure\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings.shape[0] == 108:\n",
    "    embeddings = np.delete(embeddings, -1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 256)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_male = np.array(speaker_info_loop.gender == 'M')\n",
    "all_data = (embeddings, b_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_rand = np.random.permutation(num_examples)\n",
    "idx_train = idx_rand[:100]\n",
    "idx_valid = idx_rand[100:]\n",
    "train_data =  (embeddings[idx_train], b_male[idx_train])\n",
    "valid_data = (embeddings[idx_valid], b_male[idx_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecognitionNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "        \n",
    "        #self.fc1 = nn.Linear(256, 16)\n",
    "        #self.fc2 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        #x = F.softmax(x, dim=0)\n",
    "    \n",
    "        #x = self.fc1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = F.softmax(x, dim=0)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def cuda(self, device_id=None):\n",
    "        nn.Module.cuda(self, device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 300\n",
    "epoch = 1\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecognitionNet(\n",
       "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo next\n",
    "* do the evaluate function\n",
    "* do the training function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = training_data\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, data, criterion):\n",
    "    total_correct = 0.\n",
    "    total_samples = 0.\n",
    "\n",
    "    num_samples = len(data[0])\n",
    "    all_pred = []\n",
    "    all_gt = []\n",
    "    all_correct = []\n",
    "\n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "\n",
    "\n",
    "    x_wrap = Variable(torch.from_numpy(x)).cuda() #.unsqueeze(0)\n",
    "\n",
    "    y_wrap = Variable(torch.from_numpy(y.astype('uint8')).type(torch.FloatTensor)).cuda()\n",
    "\n",
    "    output = net(x_wrap)\n",
    "\n",
    "    output.shape\n",
    "\n",
    "    loss = criterion(output, y_wrap.type(torch.cuda.LongTensor))\n",
    "\n",
    "    #total += loss.data[0]\n",
    "\n",
    "    y_pred = output.cpu().data.numpy().argmax(axis=1)\n",
    "    correct_pred = y == y_pred\n",
    "    num_correct_pred = np.sum(correct_pred)\n",
    "\n",
    "    accuracy = 1.0*num_correct_pred / num_samples\n",
    "\n",
    "        #return avg, accuracy, all_pred, all_gt, all_correct\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_male = np.full(num_examples, False)\n",
    "b_male = np.random.choice([True, False], num_examples)\n",
    "b_male = np.array(speaker_info_loop.gender == 'M')\n",
    "\n",
    "idx_rand = np.random.permutation(num_examples)\n",
    "cutoff = 50\n",
    "idx_train = idx_rand[:cutoff]\n",
    "idx_valid = idx_rand[cutoff:]\n",
    "train_data =  (embeddings[idx_train], b_male[idx_train])\n",
    "valid_data = (embeddings[idx_valid], b_male[idx_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RecognitionNet()\n",
    "net.cuda()\n",
    "net.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized: train 0.620 / validation 0.526\n",
      "Epoch 0: loss 0.689026, train 0.620 / validation 0.526\n",
      "Epoch 1: loss 0.688024, train 0.620 / validation 0.526\n",
      "Epoch 2: loss 0.687031, train 0.620 / validation 0.526\n",
      "Epoch 3: loss 0.686045, train 0.620 / validation 0.526\n",
      "Epoch 4: loss 0.685073, train 0.620 / validation 0.526\n",
      "Epoch 5: loss 0.684109, train 0.620 / validation 0.526\n",
      "Epoch 6: loss 0.683145, train 0.620 / validation 0.526\n",
      "Epoch 7: loss 0.682174, train 0.620 / validation 0.526\n",
      "Epoch 8: loss 0.681193, train 0.620 / validation 0.526\n",
      "Epoch 9: loss 0.680201, train 0.620 / validation 0.526\n",
      "Epoch 10: loss 0.679204, train 0.620 / validation 0.526\n",
      "Epoch 11: loss 0.678188, train 0.620 / validation 0.526\n",
      "Epoch 12: loss 0.677148, train 0.620 / validation 0.526\n",
      "Epoch 13: loss 0.676082, train 0.620 / validation 0.526\n",
      "Epoch 14: loss 0.674978, train 0.620 / validation 0.526\n",
      "Epoch 15: loss 0.673831, train 0.620 / validation 0.526\n",
      "Epoch 16: loss 0.672643, train 0.620 / validation 0.526\n",
      "Epoch 17: loss 0.671412, train 0.620 / validation 0.526\n",
      "Epoch 18: loss 0.670140, train 0.620 / validation 0.526\n",
      "Epoch 19: loss 0.668820, train 0.620 / validation 0.526\n",
      "Epoch 20: loss 0.667454, train 0.620 / validation 0.526\n",
      "Epoch 21: loss 0.666032, train 0.620 / validation 0.526\n",
      "Epoch 22: loss 0.664560, train 0.620 / validation 0.526\n",
      "Epoch 23: loss 0.663033, train 0.620 / validation 0.526\n",
      "Epoch 24: loss 0.661443, train 0.620 / validation 0.526\n",
      "Epoch 25: loss 0.659782, train 0.620 / validation 0.526\n",
      "Epoch 26: loss 0.658057, train 0.620 / validation 0.526\n",
      "Epoch 27: loss 0.656264, train 0.620 / validation 0.526\n",
      "Epoch 28: loss 0.654396, train 0.620 / validation 0.526\n",
      "Epoch 29: loss 0.652448, train 0.620 / validation 0.526\n",
      "Epoch 30: loss 0.650416, train 0.620 / validation 0.526\n",
      "Epoch 31: loss 0.648302, train 0.620 / validation 0.526\n",
      "Epoch 32: loss 0.646103, train 0.620 / validation 0.526\n",
      "Epoch 33: loss 0.643811, train 0.620 / validation 0.526\n",
      "Epoch 34: loss 0.641422, train 0.620 / validation 0.526\n",
      "Epoch 35: loss 0.638932, train 0.620 / validation 0.526\n",
      "Epoch 36: loss 0.636338, train 0.620 / validation 0.526\n",
      "Epoch 37: loss 0.633642, train 0.620 / validation 0.526\n",
      "Epoch 38: loss 0.630837, train 0.620 / validation 0.526\n",
      "Epoch 39: loss 0.627925, train 0.620 / validation 0.526\n",
      "Epoch 40: loss 0.624904, train 0.620 / validation 0.526\n",
      "Epoch 41: loss 0.621773, train 0.620 / validation 0.526\n",
      "Epoch 42: loss 0.618521, train 0.620 / validation 0.526\n",
      "Epoch 43: loss 0.615143, train 0.620 / validation 0.526\n",
      "Epoch 44: loss 0.611642, train 0.640 / validation 0.526\n",
      "Epoch 45: loss 0.608013, train 0.640 / validation 0.526\n",
      "Epoch 46: loss 0.604262, train 0.640 / validation 0.526\n",
      "Epoch 47: loss 0.600384, train 0.640 / validation 0.526\n",
      "Epoch 48: loss 0.596395, train 0.660 / validation 0.526\n",
      "Epoch 49: loss 0.592296, train 0.660 / validation 0.526\n",
      "Epoch 50: loss 0.588060, train 0.680 / validation 0.526\n",
      "Epoch 51: loss 0.583696, train 0.680 / validation 0.526\n",
      "Epoch 52: loss 0.579238, train 0.720 / validation 0.526\n",
      "Epoch 53: loss 0.574676, train 0.780 / validation 0.526\n",
      "Epoch 54: loss 0.570003, train 0.800 / validation 0.526\n",
      "Epoch 55: loss 0.565237, train 0.800 / validation 0.544\n",
      "Epoch 56: loss 0.560397, train 0.840 / validation 0.561\n",
      "Epoch 57: loss 0.555454, train 0.860 / validation 0.561\n",
      "Epoch 58: loss 0.550433, train 0.880 / validation 0.579\n",
      "Epoch 59: loss 0.545376, train 0.900 / validation 0.579\n",
      "Epoch 60: loss 0.540271, train 0.920 / validation 0.614\n",
      "Epoch 61: loss 0.535094, train 0.960 / validation 0.632\n",
      "Epoch 62: loss 0.529870, train 0.960 / validation 0.649\n",
      "Epoch 63: loss 0.524538, train 0.980 / validation 0.649\n",
      "Epoch 64: loss 0.519144, train 1.000 / validation 0.667\n",
      "Epoch 65: loss 0.513658, train 1.000 / validation 0.667\n",
      "Epoch 66: loss 0.508029, train 1.000 / validation 0.702\n",
      "Epoch 67: loss 0.502303, train 1.000 / validation 0.702\n",
      "Epoch 68: loss 0.496479, train 1.000 / validation 0.719\n",
      "Epoch 69: loss 0.490559, train 1.000 / validation 0.737\n",
      "Epoch 70: loss 0.484555, train 1.000 / validation 0.754\n",
      "Epoch 71: loss 0.478462, train 1.000 / validation 0.754\n",
      "Epoch 72: loss 0.472282, train 1.000 / validation 0.754\n",
      "Epoch 73: loss 0.465999, train 1.000 / validation 0.754\n",
      "Epoch 74: loss 0.459640, train 1.000 / validation 0.772\n",
      "Epoch 75: loss 0.453226, train 1.000 / validation 0.772\n",
      "Epoch 76: loss 0.446759, train 1.000 / validation 0.789\n",
      "Epoch 77: loss 0.440243, train 1.000 / validation 0.789\n",
      "Epoch 78: loss 0.433684, train 1.000 / validation 0.789\n",
      "Epoch 79: loss 0.427056, train 1.000 / validation 0.789\n",
      "Epoch 80: loss 0.420347, train 1.000 / validation 0.825\n",
      "Epoch 81: loss 0.413580, train 1.000 / validation 0.825\n",
      "Epoch 82: loss 0.406756, train 1.000 / validation 0.825\n",
      "Epoch 83: loss 0.399873, train 1.000 / validation 0.842\n",
      "Epoch 84: loss 0.392953, train 1.000 / validation 0.842\n",
      "Epoch 85: loss 0.386000, train 1.000 / validation 0.842\n",
      "Epoch 86: loss 0.379010, train 1.000 / validation 0.842\n",
      "Epoch 87: loss 0.371994, train 1.000 / validation 0.842\n",
      "Epoch 88: loss 0.364947, train 1.000 / validation 0.842\n",
      "Epoch 89: loss 0.357880, train 1.000 / validation 0.842\n",
      "Epoch 90: loss 0.350795, train 1.000 / validation 0.842\n",
      "Epoch 91: loss 0.343697, train 1.000 / validation 0.842\n",
      "Epoch 92: loss 0.336602, train 1.000 / validation 0.842\n",
      "Epoch 93: loss 0.329513, train 1.000 / validation 0.877\n",
      "Epoch 94: loss 0.322425, train 1.000 / validation 0.895\n",
      "Epoch 95: loss 0.315352, train 1.000 / validation 0.895\n",
      "Epoch 96: loss 0.308297, train 1.000 / validation 0.895\n",
      "Epoch 97: loss 0.301277, train 1.000 / validation 0.895\n",
      "Epoch 98: loss 0.294289, train 1.000 / validation 0.912\n",
      "Epoch 99: loss 0.287334, train 1.000 / validation 0.912\n",
      "Epoch 100: loss 0.280428, train 1.000 / validation 0.912\n",
      "Epoch 101: loss 0.273575, train 1.000 / validation 0.912\n",
      "Epoch 102: loss 0.266775, train 1.000 / validation 0.912\n",
      "Epoch 103: loss 0.260040, train 1.000 / validation 0.930\n",
      "Epoch 104: loss 0.253372, train 1.000 / validation 0.930\n",
      "Epoch 105: loss 0.246780, train 1.000 / validation 0.930\n",
      "Epoch 106: loss 0.240262, train 1.000 / validation 0.930\n",
      "Epoch 107: loss 0.233835, train 1.000 / validation 0.930\n",
      "Epoch 108: loss 0.227499, train 1.000 / validation 0.930\n",
      "Epoch 109: loss 0.221261, train 1.000 / validation 0.947\n",
      "Epoch 110: loss 0.215120, train 1.000 / validation 0.947\n",
      "Epoch 111: loss 0.209084, train 1.000 / validation 0.947\n",
      "Epoch 112: loss 0.203155, train 1.000 / validation 0.947\n",
      "Epoch 113: loss 0.197338, train 1.000 / validation 0.965\n",
      "Epoch 114: loss 0.191636, train 1.000 / validation 0.965\n",
      "Epoch 115: loss 0.186050, train 1.000 / validation 0.965\n",
      "Epoch 116: loss 0.180588, train 1.000 / validation 0.965\n",
      "Epoch 117: loss 0.175248, train 1.000 / validation 0.965\n",
      "Epoch 118: loss 0.170031, train 1.000 / validation 0.965\n",
      "Epoch 119: loss 0.164943, train 1.000 / validation 0.965\n",
      "Epoch 120: loss 0.159979, train 1.000 / validation 0.982\n",
      "Epoch 121: loss 0.155144, train 1.000 / validation 0.982\n",
      "Epoch 122: loss 0.150437, train 1.000 / validation 0.982\n",
      "Epoch 123: loss 0.145859, train 1.000 / validation 0.982\n",
      "Epoch 124: loss 0.141407, train 1.000 / validation 0.982\n",
      "Epoch 125: loss 0.137084, train 1.000 / validation 1.000\n",
      "Epoch 126: loss 0.132887, train 1.000 / validation 1.000\n",
      "Epoch 127: loss 0.128814, train 1.000 / validation 1.000\n",
      "Epoch 128: loss 0.124867, train 1.000 / validation 1.000\n",
      "Epoch 129: loss 0.121043, train 1.000 / validation 1.000\n",
      "Epoch 130: loss 0.117340, train 1.000 / validation 1.000\n",
      "Epoch 131: loss 0.113754, train 1.000 / validation 1.000\n",
      "Epoch 132: loss 0.110285, train 1.000 / validation 1.000\n",
      "Epoch 133: loss 0.106932, train 1.000 / validation 1.000\n",
      "Epoch 134: loss 0.103690, train 1.000 / validation 1.000\n",
      "Epoch 135: loss 0.100556, train 1.000 / validation 1.000\n",
      "Epoch 136: loss 0.097528, train 1.000 / validation 1.000\n",
      "Epoch 137: loss 0.094605, train 1.000 / validation 1.000\n",
      "Epoch 138: loss 0.091783, train 1.000 / validation 1.000\n",
      "Epoch 139: loss 0.089061, train 1.000 / validation 1.000\n",
      "Epoch 140: loss 0.086435, train 1.000 / validation 1.000\n",
      "Epoch 141: loss 0.083900, train 1.000 / validation 1.000\n",
      "Epoch 142: loss 0.081457, train 1.000 / validation 1.000\n",
      "Epoch 143: loss 0.079100, train 1.000 / validation 1.000\n",
      "Epoch 144: loss 0.076827, train 1.000 / validation 1.000\n",
      "Epoch 145: loss 0.074635, train 1.000 / validation 1.000\n",
      "Epoch 146: loss 0.072522, train 1.000 / validation 1.000\n",
      "Epoch 147: loss 0.070485, train 1.000 / validation 1.000\n",
      "Epoch 148: loss 0.068521, train 1.000 / validation 1.000\n",
      "Epoch 149: loss 0.066629, train 1.000 / validation 1.000\n",
      "Epoch 150: loss 0.064806, train 1.000 / validation 1.000\n",
      "Epoch 151: loss 0.063048, train 1.000 / validation 1.000\n",
      "Epoch 152: loss 0.061354, train 1.000 / validation 1.000\n",
      "Epoch 153: loss 0.059721, train 1.000 / validation 1.000\n",
      "Epoch 154: loss 0.058147, train 1.000 / validation 1.000\n",
      "Epoch 155: loss 0.056628, train 1.000 / validation 1.000\n",
      "Epoch 156: loss 0.055163, train 1.000 / validation 1.000\n",
      "Epoch 157: loss 0.053750, train 1.000 / validation 1.000\n",
      "Epoch 158: loss 0.052386, train 1.000 / validation 1.000\n",
      "Epoch 159: loss 0.051070, train 1.000 / validation 1.000\n",
      "Epoch 160: loss 0.049800, train 1.000 / validation 1.000\n",
      "Epoch 161: loss 0.048575, train 1.000 / validation 1.000\n",
      "Epoch 162: loss 0.047392, train 1.000 / validation 1.000\n",
      "Epoch 163: loss 0.046249, train 1.000 / validation 1.000\n",
      "Epoch 164: loss 0.045147, train 1.000 / validation 1.000\n",
      "Epoch 165: loss 0.044080, train 1.000 / validation 1.000\n",
      "Epoch 166: loss 0.043050, train 1.000 / validation 1.000\n",
      "Epoch 167: loss 0.042055, train 1.000 / validation 1.000\n",
      "Epoch 168: loss 0.041092, train 1.000 / validation 1.000\n",
      "Epoch 169: loss 0.040163, train 1.000 / validation 1.000\n",
      "Epoch 170: loss 0.039263, train 1.000 / validation 1.000\n",
      "Epoch 171: loss 0.038392, train 1.000 / validation 1.000\n",
      "Epoch 172: loss 0.037550, train 1.000 / validation 1.000\n",
      "Epoch 173: loss 0.036735, train 1.000 / validation 1.000\n",
      "Epoch 174: loss 0.035945, train 1.000 / validation 1.000\n",
      "Epoch 175: loss 0.035181, train 1.000 / validation 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176: loss 0.034440, train 1.000 / validation 1.000\n",
      "Epoch 177: loss 0.033723, train 1.000 / validation 1.000\n",
      "Epoch 178: loss 0.033028, train 1.000 / validation 1.000\n",
      "Epoch 179: loss 0.032354, train 1.000 / validation 1.000\n",
      "Epoch 180: loss 0.031700, train 1.000 / validation 1.000\n",
      "Epoch 181: loss 0.031067, train 1.000 / validation 1.000\n",
      "Epoch 182: loss 0.030452, train 1.000 / validation 1.000\n",
      "Epoch 183: loss 0.029855, train 1.000 / validation 1.000\n",
      "Epoch 184: loss 0.029276, train 1.000 / validation 1.000\n",
      "Epoch 185: loss 0.028714, train 1.000 / validation 1.000\n",
      "Epoch 186: loss 0.028168, train 1.000 / validation 1.000\n",
      "Epoch 187: loss 0.027638, train 1.000 / validation 1.000\n",
      "Epoch 188: loss 0.027123, train 1.000 / validation 1.000\n",
      "Epoch 189: loss 0.026622, train 1.000 / validation 1.000\n",
      "Epoch 190: loss 0.026135, train 1.000 / validation 1.000\n",
      "Epoch 191: loss 0.025662, train 1.000 / validation 1.000\n",
      "Epoch 192: loss 0.025202, train 1.000 / validation 1.000\n",
      "Epoch 193: loss 0.024754, train 1.000 / validation 1.000\n",
      "Epoch 194: loss 0.024318, train 1.000 / validation 1.000\n",
      "Epoch 195: loss 0.023894, train 1.000 / validation 1.000\n",
      "Epoch 196: loss 0.023481, train 1.000 / validation 1.000\n",
      "Epoch 197: loss 0.023079, train 1.000 / validation 1.000\n",
      "Epoch 198: loss 0.022688, train 1.000 / validation 1.000\n",
      "Epoch 199: loss 0.022307, train 1.000 / validation 1.000\n",
      "Epoch 200: loss 0.021935, train 1.000 / validation 1.000\n",
      "Epoch 201: loss 0.021573, train 1.000 / validation 1.000\n",
      "Epoch 202: loss 0.021220, train 1.000 / validation 1.000\n",
      "Epoch 203: loss 0.020876, train 1.000 / validation 1.000\n",
      "Epoch 204: loss 0.020540, train 1.000 / validation 1.000\n",
      "Epoch 205: loss 0.020213, train 1.000 / validation 1.000\n",
      "Epoch 206: loss 0.019894, train 1.000 / validation 1.000\n",
      "Epoch 207: loss 0.019582, train 1.000 / validation 1.000\n",
      "Epoch 208: loss 0.019278, train 1.000 / validation 1.000\n",
      "Epoch 209: loss 0.018981, train 1.000 / validation 1.000\n",
      "Epoch 210: loss 0.018691, train 1.000 / validation 1.000\n",
      "Epoch 211: loss 0.018408, train 1.000 / validation 1.000\n",
      "Epoch 212: loss 0.018131, train 1.000 / validation 1.000\n",
      "Epoch 213: loss 0.017861, train 1.000 / validation 1.000\n",
      "Epoch 214: loss 0.017597, train 1.000 / validation 1.000\n",
      "Epoch 215: loss 0.017339, train 1.000 / validation 1.000\n",
      "Epoch 216: loss 0.017086, train 1.000 / validation 1.000\n",
      "Epoch 217: loss 0.016840, train 1.000 / validation 1.000\n",
      "Epoch 218: loss 0.016599, train 1.000 / validation 1.000\n",
      "Epoch 219: loss 0.016363, train 1.000 / validation 1.000\n",
      "Epoch 220: loss 0.016132, train 1.000 / validation 1.000\n",
      "Epoch 221: loss 0.015906, train 1.000 / validation 1.000\n",
      "Epoch 222: loss 0.015685, train 1.000 / validation 1.000\n",
      "Epoch 223: loss 0.015469, train 1.000 / validation 1.000\n",
      "Epoch 224: loss 0.015257, train 1.000 / validation 1.000\n",
      "Epoch 225: loss 0.015050, train 1.000 / validation 1.000\n",
      "Epoch 226: loss 0.014847, train 1.000 / validation 1.000\n",
      "Epoch 227: loss 0.014649, train 1.000 / validation 1.000\n",
      "Epoch 228: loss 0.014454, train 1.000 / validation 1.000\n",
      "Epoch 229: loss 0.014263, train 1.000 / validation 1.000\n",
      "Epoch 230: loss 0.014077, train 1.000 / validation 1.000\n",
      "Epoch 231: loss 0.013894, train 1.000 / validation 1.000\n",
      "Epoch 232: loss 0.013714, train 1.000 / validation 1.000\n",
      "Epoch 233: loss 0.013538, train 1.000 / validation 1.000\n",
      "Epoch 234: loss 0.013366, train 1.000 / validation 1.000\n",
      "Epoch 235: loss 0.013197, train 1.000 / validation 1.000\n",
      "Epoch 236: loss 0.013031, train 1.000 / validation 1.000\n",
      "Epoch 237: loss 0.012869, train 1.000 / validation 1.000\n",
      "Epoch 238: loss 0.012709, train 1.000 / validation 1.000\n",
      "Epoch 239: loss 0.012553, train 1.000 / validation 1.000\n",
      "Epoch 240: loss 0.012399, train 1.000 / validation 1.000\n",
      "Epoch 241: loss 0.012249, train 1.000 / validation 1.000\n",
      "Epoch 242: loss 0.012101, train 1.000 / validation 1.000\n",
      "Epoch 243: loss 0.011956, train 1.000 / validation 1.000\n",
      "Epoch 244: loss 0.011814, train 1.000 / validation 1.000\n",
      "Epoch 245: loss 0.011674, train 1.000 / validation 1.000\n",
      "Epoch 246: loss 0.011537, train 1.000 / validation 1.000\n",
      "Epoch 247: loss 0.011402, train 1.000 / validation 1.000\n",
      "Epoch 248: loss 0.011270, train 1.000 / validation 1.000\n",
      "Epoch 249: loss 0.011140, train 1.000 / validation 1.000\n",
      "Epoch 250: loss 0.011012, train 1.000 / validation 1.000\n",
      "Epoch 251: loss 0.010887, train 1.000 / validation 1.000\n",
      "Epoch 252: loss 0.010763, train 1.000 / validation 1.000\n",
      "Epoch 253: loss 0.010642, train 1.000 / validation 1.000\n",
      "Epoch 254: loss 0.010523, train 1.000 / validation 1.000\n",
      "Epoch 255: loss 0.010406, train 1.000 / validation 1.000\n",
      "Epoch 256: loss 0.010291, train 1.000 / validation 1.000\n",
      "Epoch 257: loss 0.010178, train 1.000 / validation 1.000\n",
      "Epoch 258: loss 0.010067, train 1.000 / validation 1.000\n",
      "Epoch 259: loss 0.009958, train 1.000 / validation 1.000\n",
      "Epoch 260: loss 0.009850, train 1.000 / validation 1.000\n",
      "Epoch 261: loss 0.009744, train 1.000 / validation 1.000\n",
      "Epoch 262: loss 0.009640, train 1.000 / validation 1.000\n",
      "Epoch 263: loss 0.009538, train 1.000 / validation 1.000\n",
      "Epoch 264: loss 0.009438, train 1.000 / validation 1.000\n",
      "Epoch 265: loss 0.009338, train 1.000 / validation 1.000\n",
      "Epoch 266: loss 0.009241, train 1.000 / validation 1.000\n",
      "Epoch 267: loss 0.009145, train 1.000 / validation 1.000\n",
      "Epoch 268: loss 0.009051, train 1.000 / validation 1.000\n",
      "Epoch 269: loss 0.008958, train 1.000 / validation 1.000\n",
      "Epoch 270: loss 0.008867, train 1.000 / validation 1.000\n",
      "Epoch 271: loss 0.008777, train 1.000 / validation 1.000\n",
      "Epoch 272: loss 0.008688, train 1.000 / validation 1.000\n",
      "Epoch 273: loss 0.008601, train 1.000 / validation 1.000\n",
      "Epoch 274: loss 0.008515, train 1.000 / validation 1.000\n",
      "Epoch 275: loss 0.008431, train 1.000 / validation 1.000\n",
      "Epoch 276: loss 0.008348, train 1.000 / validation 1.000\n",
      "Epoch 277: loss 0.008266, train 1.000 / validation 1.000\n",
      "Epoch 278: loss 0.008185, train 1.000 / validation 1.000\n",
      "Epoch 279: loss 0.008105, train 1.000 / validation 1.000\n",
      "Epoch 280: loss 0.008027, train 1.000 / validation 1.000\n",
      "Epoch 281: loss 0.007950, train 1.000 / validation 1.000\n",
      "Epoch 282: loss 0.007874, train 1.000 / validation 1.000\n",
      "Epoch 283: loss 0.007799, train 1.000 / validation 1.000\n",
      "Epoch 284: loss 0.007725, train 1.000 / validation 1.000\n",
      "Epoch 285: loss 0.007652, train 1.000 / validation 1.000\n",
      "Epoch 286: loss 0.007580, train 1.000 / validation 1.000\n",
      "Epoch 287: loss 0.007510, train 1.000 / validation 1.000\n",
      "Epoch 288: loss 0.007440, train 1.000 / validation 1.000\n",
      "Epoch 289: loss 0.007371, train 1.000 / validation 1.000\n",
      "Epoch 290: loss 0.007303, train 1.000 / validation 1.000\n",
      "Epoch 291: loss 0.007237, train 1.000 / validation 1.000\n",
      "Epoch 292: loss 0.007171, train 1.000 / validation 1.000\n",
      "Epoch 293: loss 0.007106, train 1.000 / validation 1.000\n",
      "Epoch 294: loss 0.007042, train 1.000 / validation 1.000\n",
      "Epoch 295: loss 0.006979, train 1.000 / validation 1.000\n",
      "Epoch 296: loss 0.006916, train 1.000 / validation 1.000\n",
      "Epoch 297: loss 0.006855, train 1.000 / validation 1.000\n",
      "Epoch 298: loss 0.006794, train 1.000 / validation 1.000\n",
      "Epoch 299: loss 0.006735, train 1.000 / validation 1.000\n",
      "Epoch 300: loss 0.006676, train 1.000 / validation 1.000\n",
      "Epoch 301: loss 0.006617, train 1.000 / validation 1.000\n",
      "Epoch 302: loss 0.006560, train 1.000 / validation 1.000\n",
      "Epoch 303: loss 0.006503, train 1.000 / validation 1.000\n",
      "Epoch 304: loss 0.006447, train 1.000 / validation 1.000\n",
      "Epoch 305: loss 0.006392, train 1.000 / validation 1.000\n",
      "Epoch 306: loss 0.006338, train 1.000 / validation 1.000\n",
      "Epoch 307: loss 0.006284, train 1.000 / validation 1.000\n",
      "Epoch 308: loss 0.006231, train 1.000 / validation 1.000\n",
      "Epoch 309: loss 0.006179, train 1.000 / validation 1.000\n",
      "Epoch 310: loss 0.006127, train 1.000 / validation 1.000\n",
      "Epoch 311: loss 0.006076, train 1.000 / validation 1.000\n",
      "Epoch 312: loss 0.006025, train 1.000 / validation 1.000\n",
      "Epoch 313: loss 0.005976, train 1.000 / validation 1.000\n",
      "Epoch 314: loss 0.005927, train 1.000 / validation 1.000\n",
      "Epoch 315: loss 0.005878, train 1.000 / validation 1.000\n",
      "Epoch 316: loss 0.005830, train 1.000 / validation 1.000\n",
      "Epoch 317: loss 0.005783, train 1.000 / validation 1.000\n",
      "Epoch 318: loss 0.005736, train 1.000 / validation 1.000\n",
      "Epoch 319: loss 0.005690, train 1.000 / validation 1.000\n",
      "Epoch 320: loss 0.005645, train 1.000 / validation 1.000\n",
      "Epoch 321: loss 0.005600, train 1.000 / validation 1.000\n",
      "Epoch 322: loss 0.005555, train 1.000 / validation 1.000\n",
      "Epoch 323: loss 0.005511, train 1.000 / validation 1.000\n",
      "Epoch 324: loss 0.005468, train 1.000 / validation 1.000\n",
      "Epoch 325: loss 0.005425, train 1.000 / validation 1.000\n",
      "Epoch 326: loss 0.005383, train 1.000 / validation 1.000\n",
      "Epoch 327: loss 0.005341, train 1.000 / validation 1.000\n",
      "Epoch 328: loss 0.005299, train 1.000 / validation 1.000\n",
      "Epoch 329: loss 0.005258, train 1.000 / validation 1.000\n",
      "Epoch 330: loss 0.005218, train 1.000 / validation 1.000\n",
      "Epoch 331: loss 0.005178, train 1.000 / validation 1.000\n",
      "Epoch 332: loss 0.005139, train 1.000 / validation 1.000\n",
      "Epoch 333: loss 0.005100, train 1.000 / validation 1.000\n",
      "Epoch 334: loss 0.005061, train 1.000 / validation 1.000\n",
      "Epoch 335: loss 0.005023, train 1.000 / validation 1.000\n",
      "Epoch 336: loss 0.004985, train 1.000 / validation 1.000\n",
      "Epoch 337: loss 0.004948, train 1.000 / validation 1.000\n",
      "Epoch 338: loss 0.004911, train 1.000 / validation 1.000\n",
      "Epoch 339: loss 0.004875, train 1.000 / validation 1.000\n",
      "Epoch 340: loss 0.004839, train 1.000 / validation 1.000\n",
      "Epoch 341: loss 0.004803, train 1.000 / validation 1.000\n",
      "Epoch 342: loss 0.004768, train 1.000 / validation 1.000\n",
      "Epoch 343: loss 0.004733, train 1.000 / validation 1.000\n",
      "Epoch 344: loss 0.004699, train 1.000 / validation 1.000\n",
      "Epoch 345: loss 0.004665, train 1.000 / validation 1.000\n",
      "Epoch 346: loss 0.004631, train 1.000 / validation 1.000\n",
      "Epoch 347: loss 0.004598, train 1.000 / validation 1.000\n",
      "Epoch 348: loss 0.004565, train 1.000 / validation 1.000\n",
      "Epoch 349: loss 0.004533, train 1.000 / validation 1.000\n",
      "Epoch 350: loss 0.004500, train 1.000 / validation 1.000\n",
      "Epoch 351: loss 0.004469, train 1.000 / validation 1.000\n",
      "Epoch 352: loss 0.004437, train 1.000 / validation 1.000\n",
      "Epoch 353: loss 0.004406, train 1.000 / validation 1.000\n",
      "Epoch 354: loss 0.004375, train 1.000 / validation 1.000\n",
      "Epoch 355: loss 0.004345, train 1.000 / validation 1.000\n",
      "Epoch 356: loss 0.004314, train 1.000 / validation 1.000\n",
      "Epoch 357: loss 0.004285, train 1.000 / validation 1.000\n",
      "Epoch 358: loss 0.004255, train 1.000 / validation 1.000\n",
      "Epoch 359: loss 0.004226, train 1.000 / validation 1.000\n",
      "Epoch 360: loss 0.004197, train 1.000 / validation 1.000\n",
      "Epoch 361: loss 0.004168, train 1.000 / validation 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362: loss 0.004140, train 1.000 / validation 1.000\n",
      "Epoch 363: loss 0.004112, train 1.000 / validation 1.000\n",
      "Epoch 364: loss 0.004084, train 1.000 / validation 1.000\n",
      "Epoch 365: loss 0.004057, train 1.000 / validation 1.000\n",
      "Epoch 366: loss 0.004029, train 1.000 / validation 1.000\n",
      "Epoch 367: loss 0.004002, train 1.000 / validation 1.000\n",
      "Epoch 368: loss 0.003976, train 1.000 / validation 1.000\n",
      "Epoch 369: loss 0.003949, train 1.000 / validation 1.000\n",
      "Epoch 370: loss 0.003923, train 1.000 / validation 1.000\n",
      "Epoch 371: loss 0.003898, train 1.000 / validation 1.000\n",
      "Epoch 372: loss 0.003872, train 1.000 / validation 1.000\n",
      "Epoch 373: loss 0.003847, train 1.000 / validation 1.000\n",
      "Epoch 374: loss 0.003822, train 1.000 / validation 1.000\n",
      "Epoch 375: loss 0.003797, train 1.000 / validation 1.000\n",
      "Epoch 376: loss 0.003772, train 1.000 / validation 1.000\n",
      "Epoch 377: loss 0.003748, train 1.000 / validation 1.000\n",
      "Epoch 378: loss 0.003724, train 1.000 / validation 1.000\n",
      "Epoch 379: loss 0.003700, train 1.000 / validation 1.000\n",
      "Epoch 380: loss 0.003676, train 1.000 / validation 1.000\n",
      "Epoch 381: loss 0.003653, train 1.000 / validation 1.000\n",
      "Epoch 382: loss 0.003630, train 1.000 / validation 1.000\n",
      "Epoch 383: loss 0.003607, train 1.000 / validation 1.000\n",
      "Epoch 384: loss 0.003584, train 1.000 / validation 1.000\n",
      "Epoch 385: loss 0.003561, train 1.000 / validation 1.000\n",
      "Epoch 386: loss 0.003539, train 1.000 / validation 1.000\n",
      "Epoch 387: loss 0.003517, train 1.000 / validation 1.000\n",
      "Epoch 388: loss 0.003495, train 1.000 / validation 1.000\n",
      "Epoch 389: loss 0.003474, train 1.000 / validation 1.000\n",
      "Epoch 390: loss 0.003452, train 1.000 / validation 1.000\n",
      "Epoch 391: loss 0.003431, train 1.000 / validation 1.000\n",
      "Epoch 392: loss 0.003410, train 1.000 / validation 1.000\n",
      "Epoch 393: loss 0.003389, train 1.000 / validation 1.000\n",
      "Epoch 394: loss 0.003368, train 1.000 / validation 1.000\n",
      "Epoch 395: loss 0.003348, train 1.000 / validation 1.000\n",
      "Epoch 396: loss 0.003327, train 1.000 / validation 1.000\n",
      "Epoch 397: loss 0.003307, train 1.000 / validation 1.000\n",
      "Epoch 398: loss 0.003287, train 1.000 / validation 1.000\n",
      "Epoch 399: loss 0.003268, train 1.000 / validation 1.000\n",
      "Epoch 400: loss 0.003248, train 1.000 / validation 1.000\n",
      "Epoch 401: loss 0.003229, train 1.000 / validation 1.000\n",
      "Epoch 402: loss 0.003209, train 1.000 / validation 1.000\n",
      "Epoch 403: loss 0.003190, train 1.000 / validation 1.000\n",
      "Epoch 404: loss 0.003172, train 1.000 / validation 1.000\n",
      "Epoch 405: loss 0.003153, train 1.000 / validation 1.000\n",
      "Epoch 406: loss 0.003134, train 1.000 / validation 1.000\n",
      "Epoch 407: loss 0.003116, train 1.000 / validation 1.000\n",
      "Epoch 408: loss 0.003098, train 1.000 / validation 1.000\n",
      "Epoch 409: loss 0.003080, train 1.000 / validation 1.000\n",
      "Epoch 410: loss 0.003062, train 1.000 / validation 1.000\n",
      "Epoch 411: loss 0.003044, train 1.000 / validation 1.000\n",
      "Epoch 412: loss 0.003027, train 1.000 / validation 1.000\n",
      "Epoch 413: loss 0.003009, train 1.000 / validation 1.000\n",
      "Epoch 414: loss 0.002992, train 1.000 / validation 1.000\n",
      "Epoch 415: loss 0.002975, train 1.000 / validation 1.000\n",
      "Epoch 416: loss 0.002958, train 1.000 / validation 1.000\n",
      "Epoch 417: loss 0.002941, train 1.000 / validation 1.000\n",
      "Epoch 418: loss 0.002924, train 1.000 / validation 1.000\n",
      "Epoch 419: loss 0.002908, train 1.000 / validation 1.000\n",
      "Epoch 420: loss 0.002892, train 1.000 / validation 1.000\n",
      "Epoch 421: loss 0.002875, train 1.000 / validation 1.000\n",
      "Epoch 422: loss 0.002859, train 1.000 / validation 1.000\n",
      "Epoch 423: loss 0.002843, train 1.000 / validation 1.000\n",
      "Epoch 424: loss 0.002827, train 1.000 / validation 1.000\n",
      "Epoch 425: loss 0.002812, train 1.000 / validation 1.000\n",
      "Epoch 426: loss 0.002796, train 1.000 / validation 1.000\n",
      "Epoch 427: loss 0.002781, train 1.000 / validation 1.000\n",
      "Epoch 428: loss 0.002765, train 1.000 / validation 1.000\n",
      "Epoch 429: loss 0.002750, train 1.000 / validation 1.000\n",
      "Epoch 430: loss 0.002735, train 1.000 / validation 1.000\n",
      "Epoch 431: loss 0.002720, train 1.000 / validation 1.000\n",
      "Epoch 432: loss 0.002705, train 1.000 / validation 1.000\n",
      "Epoch 433: loss 0.002691, train 1.000 / validation 1.000\n",
      "Epoch 434: loss 0.002676, train 1.000 / validation 1.000\n",
      "Epoch 435: loss 0.002662, train 1.000 / validation 1.000\n",
      "Epoch 436: loss 0.002647, train 1.000 / validation 1.000\n",
      "Epoch 437: loss 0.002633, train 1.000 / validation 1.000\n",
      "Epoch 438: loss 0.002619, train 1.000 / validation 1.000\n",
      "Epoch 439: loss 0.002605, train 1.000 / validation 1.000\n",
      "Epoch 440: loss 0.002591, train 1.000 / validation 1.000\n",
      "Epoch 441: loss 0.002577, train 1.000 / validation 1.000\n",
      "Epoch 442: loss 0.002564, train 1.000 / validation 1.000\n",
      "Epoch 443: loss 0.002550, train 1.000 / validation 1.000\n",
      "Epoch 444: loss 0.002537, train 1.000 / validation 1.000\n",
      "Epoch 445: loss 0.002524, train 1.000 / validation 1.000\n",
      "Epoch 446: loss 0.002510, train 1.000 / validation 1.000\n",
      "Epoch 447: loss 0.002497, train 1.000 / validation 1.000\n",
      "Epoch 448: loss 0.002484, train 1.000 / validation 1.000\n",
      "Epoch 449: loss 0.002471, train 1.000 / validation 1.000\n",
      "Epoch 450: loss 0.002458, train 1.000 / validation 1.000\n",
      "Epoch 451: loss 0.002446, train 1.000 / validation 1.000\n",
      "Epoch 452: loss 0.002433, train 1.000 / validation 1.000\n",
      "Epoch 453: loss 0.002421, train 1.000 / validation 1.000\n",
      "Epoch 454: loss 0.002408, train 1.000 / validation 1.000\n",
      "Epoch 455: loss 0.002396, train 1.000 / validation 1.000\n",
      "Epoch 456: loss 0.002384, train 1.000 / validation 1.000\n",
      "Epoch 457: loss 0.002372, train 1.000 / validation 1.000\n",
      "Epoch 458: loss 0.002360, train 1.000 / validation 1.000\n",
      "Epoch 459: loss 0.002348, train 1.000 / validation 1.000\n",
      "Epoch 460: loss 0.002336, train 1.000 / validation 1.000\n",
      "Epoch 461: loss 0.002324, train 1.000 / validation 1.000\n",
      "Epoch 462: loss 0.002312, train 1.000 / validation 1.000\n",
      "Epoch 463: loss 0.002301, train 1.000 / validation 1.000\n",
      "Epoch 464: loss 0.002289, train 1.000 / validation 1.000\n",
      "Epoch 465: loss 0.002278, train 1.000 / validation 1.000\n",
      "Epoch 466: loss 0.002267, train 1.000 / validation 1.000\n",
      "Epoch 467: loss 0.002255, train 1.000 / validation 1.000\n",
      "Epoch 468: loss 0.002244, train 1.000 / validation 1.000\n",
      "Epoch 469: loss 0.002233, train 1.000 / validation 1.000\n",
      "Epoch 470: loss 0.002222, train 1.000 / validation 1.000\n",
      "Epoch 471: loss 0.002211, train 1.000 / validation 1.000\n",
      "Epoch 472: loss 0.002200, train 1.000 / validation 1.000\n",
      "Epoch 473: loss 0.002190, train 1.000 / validation 1.000\n",
      "Epoch 474: loss 0.002179, train 1.000 / validation 1.000\n",
      "Epoch 475: loss 0.002168, train 1.000 / validation 1.000\n",
      "Epoch 476: loss 0.002158, train 1.000 / validation 1.000\n",
      "Epoch 477: loss 0.002148, train 1.000 / validation 1.000\n",
      "Epoch 478: loss 0.002137, train 1.000 / validation 1.000\n",
      "Epoch 479: loss 0.002127, train 1.000 / validation 1.000\n",
      "Epoch 480: loss 0.002117, train 1.000 / validation 1.000\n",
      "Epoch 481: loss 0.002107, train 1.000 / validation 1.000\n",
      "Epoch 482: loss 0.002097, train 1.000 / validation 1.000\n",
      "Epoch 483: loss 0.002087, train 1.000 / validation 1.000\n",
      "Epoch 484: loss 0.002077, train 1.000 / validation 1.000\n",
      "Epoch 485: loss 0.002067, train 1.000 / validation 1.000\n",
      "Epoch 486: loss 0.002057, train 1.000 / validation 1.000\n",
      "Epoch 487: loss 0.002047, train 1.000 / validation 1.000\n",
      "Epoch 488: loss 0.002038, train 1.000 / validation 1.000\n",
      "Epoch 489: loss 0.002028, train 1.000 / validation 1.000\n",
      "Epoch 490: loss 0.002019, train 1.000 / validation 1.000\n",
      "Epoch 491: loss 0.002009, train 1.000 / validation 1.000\n",
      "Epoch 492: loss 0.002000, train 1.000 / validation 1.000\n",
      "Epoch 493: loss 0.001991, train 1.000 / validation 1.000\n",
      "Epoch 494: loss 0.001981, train 1.000 / validation 1.000\n",
      "Epoch 495: loss 0.001972, train 1.000 / validation 1.000\n",
      "Epoch 496: loss 0.001963, train 1.000 / validation 1.000\n",
      "Epoch 497: loss 0.001954, train 1.000 / validation 1.000\n",
      "Epoch 498: loss 0.001945, train 1.000 / validation 1.000\n",
      "Epoch 499: loss 0.001936, train 1.000 / validation 1.000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "train_losses = np.zeros(num_epochs)\n",
    "train_accuracy = np.zeros(num_epochs)\n",
    "valid_accuracy = np.zeros(num_epochs)\n",
    "\n",
    "print \"Initialized: train %0.3f / validation %0.3f\" % (evaluate(net, train_data, criterion), evaluate(net, valid_data, criterion))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    num_samples = len(train_data[0])\n",
    "    all_pred = []\n",
    "    all_gt = []\n",
    "    all_correct = []\n",
    "\n",
    "    x = train_data[0]\n",
    "    y = train_data[1]\n",
    "\n",
    "    x_wrap = Variable(torch.from_numpy(x)).cuda() #.unsqueeze(0)\n",
    "\n",
    "    y_wrap = Variable(torch.from_numpy(y.astype('uint8')).type(torch.FloatTensor)).cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward\n",
    "    output = net(x_wrap)  \n",
    "\n",
    "    loss = criterion(output, y_wrap.type(torch.cuda.LongTensor))\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    #print loss.cpu().data.numpy()[0]\n",
    "    this_train_accuracy = evaluate(net, train_data, criterion)\n",
    "    this_valid_accuracy = evaluate(net, valid_data, criterion)\n",
    "    train_accuracy[epoch] = this_train_accuracy\n",
    "    valid_accuracy[epoch] = this_valid_accuracy\n",
    "    print \"Epoch %d: loss %0.6f, train %0.3f / validation %0.3f\" % (epoch, loss.cpu().data.numpy()[0], this_train_accuracy, this_valid_accuracy)\n",
    "\n",
    "    \n",
    "    #avg = total / len(train_loader)\n",
    "    #train_losses.append(avg)\n",
    "\n",
    "\n",
    "#logging.info('====> Train set loss: {:.4f}'.format(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHypJREFUeJzt3XtwXOWd5vHvz7JkXSws+YJsLIPMxME2l2BQwCyzWSU7yRpIIJPEATZUhqlkXZOChGQnO+XUZDNMaiaXSmYzyw4h482yrsmSeIizSbwpZ024dLyZAoKdgJEvGBsMlg1YEmrb6m65W93v/tFHQhZyqy/n9Gl1P58q1+k+fbr7fWXx+OV33vMec84hIiLVZVbYDRAREf8p3EVEqpDCXUSkCincRUSqkMJdRKQKKdxFRKqQwl1EpAop3EVEqpDCXUSkCs0O64sXLlzourq6inpvLBajpaXF3wZVOPW5NqjPtaGUPu/evXvAObdouuNCC/euri527dpV1HsjkQg9PT3+NqjCqc+1QX2uDaX02cxeyec4lWVERKqQwl1EpAop3EVEqpDCXUSkCincRUSq0LThbmYPmtkJM+s9x+tmZveZ2SEz22NmV/nfTBERKUQ+I/fNwLocr98ArPD+bAAeKL1ZIiJSimnnuTvndppZV45DbgH+yWXv1/eUmbWZ2RLn3Gs+tXHGGBg+ww+ffpXRdMb3zz7ySpLfJV8o+H2zMqOseW0Lc9Ix39sUtJGTUZ46tC3sZpSV+lwbTjW/E+gJ9Dv8uIhpKXB0wvM+b9/bwt3MNpAd3dPR0UEkEinqC4eHh4t+b5B2HEnxowNJAMz3T3dw+FDB7+q2A3x+zn0AZJz/rQrcqbAbEAL1uer9rP3OwDOsrFeoOuc2AZsAuru7XbFXaFXqFW27drxA3cHDHPrbGzDzN0iL7vO+UXgY+LPfMGvx5b62KWiV+vccJPW5NswvQ5/9mC1zDFg24Xmnt6/mDMWTzGuq9z3YSxIbyG6bF4bbDhEpKz/CfRvwSW/WzFrgZC3W2wGi8RRtzfVhN+Ns8cHstnlBuO0QkbKatixjZj8iW/lfaGZ9wF8B9QDOue8B24EbgUNAHPjToBpb6aKJJO3NDWE342yxfmicB7MrrF0iEqh8ZsvcPs3rDrjLtxbNYEOxFEvmNYbdjLPFBlSSEalBoS35W42i8SSrlpwXdjOyBg9DKg7RV6FF4S5SaxTuPoomUrRXQs29bzd8/31vPb/0j8Nri4iEQuHukzOjaeLJdGWcUB16Obu98dvQuhg6rwm3PSJSdgp3n0TjKQDaKuGE6tj0x0s/Ai2aJSNSi7QqpE+G4tkrUytitkysH2wWNLWH3RIRCYnC3SdjI/eKqLnHB7Lz2mfpr1ekVum/fp9EvZH7vEoId01/FKl5CnefDI2P3CugLBMf1PRHkRqnE6o+iYYZ7vt/Ab/6z+C8pYZPHoOVN5W/HSJSMRTuPonGkzTMnkVjfQj/M3T4MTj1Gqy+Oft8GXDVJ8vfDhGpGAp3H5w4PcI/7nyJRa1zwlkRMjYAbRfCRzaV/7tFpCKp5u6DA6+dBuD6PwhpTnlsAFoWhfPdIlKRFO4+SKTSAHz6X18cTgPiA7pYSUTOonD3wYgX7o31deE0QFMfRWQShbsPzqSys1RCOZmaHoXEmyrLiMhZdELVB2NlmaZyj9x//xC89mz2sea1i8gECncfhFKWyaRh293ZNWQa22DJleX7bhGpeAp3H4yMl2XKGO7xN7MXLa37Jly7oXzfKyIzgmruPkik0jTUzaJuVhnnuMe9ZX01S0ZEpqBw98FIKs2ccp9MjfVntzqRKiJTULj7YCSVLv80yLEbcmgKpIhMQeHug5FUuvwzZeKD2a1myYjIFHRC1QcjqUx55rgnY/DG3uzj1/cABk3zg/9eEZlxFO4+SJSrLPPIl2HXg289b70A6vRXKCJvp2TwQdlq7kOvwMJ3wrqvZ5+3Lw/+O0VkRlK4+2BkNMO8pjLcXi8+AO1d8I4/Cv67RGRG0wlVH4wk0zTOLsOPUkv7ikieFO4+SKTSNDUEXJZxzlv9URcticj0FO4+OJlIBV6WqUsnIH1GI3cRyYvCvUTpjOPUSIq2gMO9PnUy+0Dz2kUkDzqhWqJTiRTOQVtzQzBf8Ju/h6ce4Koz8exzXZEqInlQuJdoKJ4EoL0loJH7oUcBGFi4lgsuWgEX/atgvkdEqkpeZRkzW2dmL5jZITPbOMXrF5nZY2a2x8wiZtbpf1Mr01A8BQQ4co8PQmc3By+5C9Z9DebMDeZ7RKSqTBvuZlYH3A/cAKwGbjez1ZMO+zbwT865K4CvAl/3u6GV6mQiO3IPrOauGTIiUoR8Ru7XAIeccy8555LAFuCWScesBh73Hj8xxetVayiWHbm3BzFyz2SyI3fNkBGRAuUT7kuBoxOe93n7JnoO+Ij3+I+BVjOrieHmeM09iHAfiYJLa4aMiBTMrxOqXwT+wczuBHYCx4D05IPMbAOwAaCjo4NIJFLUlw0PDxf9Xr8992ISA3Y//Rtmmb93YmqO9XENsO+VEwy3LKuYPpdLJf09l4v6XBvK0ed8wv0YsGzC805v3zjn3HG8kbuZzQU+6pyLTv4g59wmYBNAd3e36+npKarRkUiEYt/rt0ejz9P22mu8773v9e9DMxnY+S0Y7gVgdfd7OHHUKqbP5VJJf8/loj7XhnL0OZ+yzDPACjNbbmYNwG3AtokHmNlCMxv7rC8BD1IjhuIp/0sybx6GyNfg8OPZhcLOn3z+WkQkt2nD3Tk3CtwN7AD2Aw875/aa2VfN7GbvsB7gBTM7CHQAfxtQeytONJ6krdnnmTJj90e99Qdwz3PQ2uHv54tI1cur5u6c2w5sn7TvKxMebwW2+tu0mSEaT9FxXqO/H6qbX4tIibS2TImi8VQAI3fd/FpESqNwL9FQPOl/zX3s5te6eElEiqRwL8GZ0TTxZJr2IEbujfNgdkBLGohI1VO4l+Ckt67MPL9H7rF+lWREpCQK9xKMLRrm+8g98aZKMiJSEoV7CaJBLT2QjEFDi7+fKSI1ReFegrGRu++32EvGFe4iUhKFewnGR+4tPo/cUzGob/b3M0WkpijcSxBNBFRzTyWgvsnfzxSRmqJwL8FQPEnD7Fk01df5+8Eqy4hIiRTuJYjGUrQ11WN+LvXrnMoyIlIyhXsJookArk5NJ8FlVJYRkZIo3EswFMS6MslYdquyjIiUQOFegkCW+03Fs1uVZUSkBAr3EgRyo45UIrtVuItICRTuRXLOcTKeoi2Iq1MBGhTuIlI8hXuR4sk0yXQmgDnuKsuISOkU7kUa8q5OVc1dRCpRXrfZk7eLeuvKlFSWee6f4eWdZ+87eTS7VVlGREqgcC9SdHy53xLCPfI1GD4BTfPP3r/4cmi7qITWiUitU7gXafhMNtznzinhRxgbgKvvhHVf96dRIiIe1dyLNJLKANDUUOS6MqkEJIehRXdcEhH/KdyLlEilAWisL/JHGBvIbnU7PREJgMK9SCNj4T67yJF7rD+7bVnkU4tERN6icC9SyWWZ+GB2q7KMiARA4V6ksbLMnNlF/Ahjg/Dyr7OPdSNsEQmAZssU6UwqzZzZs4pby/2Rv4TnfgR1c6B1sf+NE5Gap5F7kRKpdGklmYWXwGd3aWlfEQmEwr1II6l08SdTk/Fsrb3tQn8bJSLiUbgXaSSVKWGOu26jJyLBUrgXKeHV3IuSSug2eiISKIV7kUZSaRrrSyjLqNYuIgFSuBdpJJWmqdhwV1lGRAKWV7ib2Toze8HMDpnZxilev9DMnjCz35vZHjO70f+mVpaRVKb4pQeScZVlRCRQ06aTmdUB9wM3AKuB281s9aTDvgw87JxbA9wGfNfvhlaaossymQyMJlSWEZFA5TP0vAY45Jx7yTmXBLYAt0w6xgHneY/nAcf9a2JlOplIFVeWGdUNsEUkePlcoboUODrheR9w7aRj7gUeMbPPAi3AH/nSugr10NOvcOL0GZrnFBHuSe82ehq5i0iA/Fp+4HZgs3Pu78zsOuAHZnaZcy4z8SAz2wBsAOjo6CASiRT1ZcPDw0W/1w+P7T0DwLsa+gtuR2PiDdYCBw6/wuvx/N8bdp/DoD7XBvU5GPmE+zFg2YTnnd6+iT4FrANwzj1pZo3AQuDExIOcc5uATQDd3d2up6enqEZHIhGKfa8fth7/HRcnTrH+xiLacGI/PA0rL7+KlZfl//6w+xwG9bk2qM/ByKfm/gywwsyWm1kD2ROm2yYd8yrwbwHMbBXQCPT72dBKEo2nmNdcX9ybVZYRkTKYduTunBs1s7uBHUAd8KBzbq+ZfRXY5ZzbBvw58N/N7AtkT67e6ZxzQTY8TEPxJB3nNRb2pkwGHvkyvPF89rmmQopIgPKquTvntgPbJ+37yoTH+4Dr/W1a5YrGU1zS0VrYm4Zehqfuh7mLYcmVsGhlMI0TEUHruRclGk/S1txQ2JvG7rx0yz/Aivf73ygRkQm0/ECBkqMZYsk07YXW3MduiK3b6olIGSjcCxRNJAFoKzjcvfPLzQp3EQmewr1AsTPZe6e2zCmwohXXyF1EykfhXqBUOntdVkOha7nHBqFhrmbJiEhZKNwLlBzNhnt9XQE/unQK9v0MmhcE1CoRkbMp3AtU1Mj9xV/BqWPQ1B5Qq0REzqZwL9DYyL2hkJH7KW+1hvWb/W+QiMgUFO4FSqWzF94WVJYZmwY5b1nu40REfKJwL9BYWaa+zvJ/U3wgW5Kp0zVjIlIeCvcCJYupuccGoGVRQC0SEXk7hXuBxk+oFlqW0cVLIlJGCvcCFTQV0rnsapDxAWjRNEgRKR8VgQs0XnPPpyzz87vh2f+VfXxRzSyaKSIVQOFeoKQ3Wyavssyx3bBoFVz2EbjsowG3TETkLQr3AqUKmece64dVH4J/8xcBt0pE5GyquRcoOV6WmWYqZCYDiTe1UJiIhELhXqBUvidUE0PgMpolIyKhULgXaOyE6uxZ04zcx9Zv18hdREKgcC9QMu1omD0Ls2nCXeu3i0iIFO4FSqUzeZ5M9cJdZRkRCYHCvUDJ0Ux+68qMj9y17ICIlJ/CvUCpdCa/q1PHR+7zg22QiMgUFO4FSqYz+S0aFhuAxjaoK/BG2iIiPlC4FyiVdvnV3ONaCVJEwqNwL1ByNJ1/WUYzZUQkJAr3AqXSbvqrU8Fb5lcrQYpIOBTuBcp7KqTKMiISIoV7gc6k8pgtk8lAfFBlGREJjcK9QCOjaZoa6nIfpHVlRCRkCvcCJZJpmuqnCXctPSAiIVO4F2hkNE3jdOGuRcNEJGQK9wKNpDI01k/zY9O6MiISsrzC3czWmdkLZnbIzDZO8fp3zOxZ789BM4v639TKMJLMY+SudWVEJGTT3mbPzOqA+4H3A33AM2a2zTm3b+wY59wXJhz/WWBNAG2tCHmVZU4dz261royIhCSfkfs1wCHn3EvOuSSwBbglx/G3Az/yo3GVZjSdIZV2uU+onuyD//d3UN+sdWVEJDT53CB7KXB0wvM+4NqpDjSzi4DlwOPneH0DsAGgo6ODSCRSSFvHDQ8PF/3eUiRGHQDHXj1CJHJsymPmRfeyBnh18b/jJR/bGFafw6Q+1wb1ORj5hHshbgO2OufSU73onNsEbALo7u52PT09RX1JJBKh2PeWYmD4DDz6KJeuXEHPdV1TH3RoFJ6FC9//GS68cMp/A4sSVp/DpD7XBvU5GPmUZY4ByyY87/T2TeU2qrQkAzCSyv6blbPmnoxntw3NZWiRiMjU8gn3Z4AVZrbczBrIBvi2yQeZ2UqgHXjS3yZWjrzCPeWFe73CXUTCM224O+dGgbuBHcB+4GHn3F4z+6qZ3Tzh0NuALc45F0xTwzeSygDkPqGqcBeRCpBXzd05tx3YPmnfVyY9v9e/ZlWmt0buOf5NVFlGRCqArlAtQEJlGRGZIRTuBci7LDOrXnPcRSRUCvcC3PXD3wHkXvI3GVdJRkRCp3DPUyqdITma4YJ5jSxf0JLjwBjU53hdRKQMFO55isZTAPxZzx8wa1aOe6imEhq5i0joFO55isaTALQ1N+Q+MBmH+qYytEhE5NwU7nmKJrIj9/bmaU6UpuIqy4hI6BTueRqKeSP3phwj95FT8PKvVZYRkdAp3PM0VnNvyzVyf25Ldtu6pAwtEhE5N4V7nqKJ7Mi9vSXHyP30a9ntzf+tDC0SETk3hXsekqMZvv3IQcygJdcc9/gAzO2AWdPcqUlEJGAK9zzse+0UydEMXQtaMMsxDTI2qJtii0hFULjnYcibBvnt9e/KfWCsH1oWlKFFIiK5KdzzMDbHfdppkPEBaFlUhhaJiOSmcM/DWzNlcpxMdQ7efFllGRGpCAr3PAzFU5jBvKYcI/dHvgw4aO0oW7tERM5F4Z6HaDzJeY311OVaU6b/QHZ79Z+Wp1EiIjko3PMwFE9NX2+PDcCKD0Dz/PI0SkQkB4V7DqdGUuw82M8rg7HpFwyLDajeLiIVI697qNaqb/7yAA89/SoAN12eY0kB57yZMpoGKSKVQeGew+snR7h4YQvfWn8FKzpaz31gMgajI5oGKSIVQ+Gew1A8yZK2Rq6+aJo6eqw/u1VZRkQqhGruOUQTqelr7QDxwey2ReEuIpVB4Z5DNJ6iLdfc9jGxgexWI3cRqRAK93PIZBzReJL2fEbuY2UZjdxFpEIo3M/h9JlRMm6am3OMiXsjd4W7iFQIhfs55H1DbMiWZWY3QYPunSoilWHGzZbZ/C8v863HYsz+9SOBfk864wCY35JnzV2jdhGpIDMu3Fd0tHLdktl0di4N/LuaGuq4dnkeFybFFe4iUllmXLhf/46FpPrm0NNzadhNeUusH1rOD7sVIiLjVHP3Q2JIC4aJSEVRuPshGYf65rBbISIyTuHuh1RcM2VEpKLkFe5mts7MXjCzQ2a28RzHfNzM9pnZXjP7ob/NrGDOZcNdI3cRqSDTnlA1szrgfuD9QB/wjJltc87tm3DMCuBLwPXOuSEzq52zi6lEdlvfFG47REQmyGfkfg1wyDn3knMuCWwBbpl0zH8A7nfODQE4507428wKlopntyrLiEgFyWcq5FLg6ITnfcC1k455J4CZ/QtQB9zrnPu/kz/IzDYAGwA6OjqIRCJFNBmGh4eLfq/f5oyc4DrgwEtHeT0RCex7KqnP5aI+1wb1ORh+zXOfDawAeoBOYKeZXe6ci048yDm3CdgE0N3d7Xp6eor6skgkQrHv9d2JA/AUrLzsSlZe3hPY11RUn8tEfa4N6nMw8inLHAOWTXje6e2bqA/Y5pxLOedeBg6SDfvqp7KMiFSgfEbuzwArzGw52VC/Dfj3k475GXA78D/NbCHZMs1Lfja0Yo2Fu2bLiAQulUrR19fHyMhI2E0pybx589i/f3/OYxobG+ns7KS+Po/1raYwbbg750bN7G5gB9l6+oPOub1m9lVgl3Num/faB8xsH5AG/pNzbrCoFs00SYW7SLn09fXR2tpKV1cXZhZ2c4p2+vRpWlvPfV9m5xyDg4P09fWxfPnyor4jr5q7c247sH3Svq9MeOyA/+j9qS3jZRmFu0jQRkZGZnyw58PMWLBgAf39/UV/hq5QLZXKMiJlVe3BPqbUfircS5WMZbcKd5GqF41G+e53v1vw+2688Uai0ej0B/pI4V6K9Chs/2L2scoyIlXvXOE+Ojqa833bt2+nra0tqGZNacat515RTnrXds2/GBrmhtsWEQncxo0bOXz4MFdeeSX19fU0NjbS3t7OgQMHOHjwIB/+8Ic5evQoIyMj3HPPPWzYsAGArq4udu3axfDwMDfccAPXXnstzzzzDEuXLuXnP/85TU3+L1+icC9F3JsQtO4bUCN1QJFK8df/Zy/7jp/y9TNXX3Aef/Whc98I6Bvf+Aa9vb08++yzRCIRbrrpJnp7e8dntDz44IPMnz+fRCLBu9/9bj760Y+yYMHZd3N78cUX+f73v8/mzZv5+Mc/zk9+8hPuuOMOX/sBCvfSxAay22bdYk+kFl1zzTVnTVW87777+OlPfwrA0aNHefHFF98W7suXL+eKK64A4Oqrr+bIkSOBtE3hXoqYN02pJY/7rIqIr3KNsMulpeWtK9MjkQiPPvooTz75JM3NzfT09Ex5sdWcOXPGH9fV1ZFIJAJpm06oliLujdxbFoXbDhEpi9bWVk6fPj3laydPnqS9vZ3m5mYOHDjAU089VebWnU0j91LEBmB2k9aVEakRCxYs4Prrr+eyyy6jqamJjo6O8dfWrVvH9773PVatWsUll1zC2rVrQ2ypwr14yTj8dhPM7Zj+WBGpGj/84dQ3mpszZw6//OUvp3xtrK6+cOFCent7x0f/X/ziFwNpI6gsU7zDj0M6CeddEHZLRETeRuFerOE3stv1m0NthojIVBTuxdI0SBGpYAr3YsUHoHEezG4IuyUiIm+jcC9WbECjdhGpWAr3YsX6Nb9dRCqWwr1Y8UFo0chdRM5t7tzsgoLHjx/nYx/72JTH9PT0sGvXLt+/W+FerNgANGvZARGZ3gUXXMDWrVvL+p26iKkYmYw3cldZRqSWbNy4kWXLlnHXXXcBcO+99zJ79myeeOIJhoaGSKVS/M3f/A233HLLWe87cuQIH/zgB+nt7SWRSHDnnXeyb98+Vq5cGdjaMgr3YoxEwaVVlhEJ0y83wuvP+/uZiy+HG75xzpdvvfVWPv/5z4+H+8MPP8yOHTv43Oc+x3nnncfAwABr167l5ptvPudt8h544AGam5vZv38/e/bs4aqrrvK3Dx6FezHGVoPUbBmRmrJmzRpOnDjB8ePH6e/vp729ncWLF/OFL3yBnTt3MmvWLI4dO8Ybb7zB4sWLp/yMnTt38ulPfxqAK664Ynz5X78p3IsxdgGTRu4i4ckxwg7S+vXr2bp1K6+//jq33norDz30EP39/ezevZv6+nq6urqmXOq33HRCtRhxhbtIrbr11lvZsmULW7duZf369Zw8eZLzzz+f+vp6nnjiCV555ZWc73/Pe97Dj3/8YwB6e3vZs2dPIO3UyL0Y4zfp0AlVkVpz6aWXcvr0aZYuXcqSJUv4xCc+wYc+9CEuv/xyuru7WblyZc73f+Yzn+GOO+5g1apVrFq1iquvvjqQds68cP/dD3j3b78Je0NcQ33s3qmaCilSk55//q0TuQsXLuTJJ5+c8rjh4WEge4Ps3t5eAJqamti8eTOtra2BtnHmhXvzfGIty2hZFPKo+fzVUFcfbhtERM5h5oX7ypvY93oL5/f0hN0SEZGKpROqIiJVSOEuIjOKcy7sJpRFqf1UuIvIjNHY2Mjg4GDVB7xzjsHBQRobG4v+jJlXcxeRmtXZ2UlfXx/9/f1hN6UkIyMj0wZ3Y2MjnZ2dRX+Hwl1EZoz6+nqWL18edjNKFolEWLNmTaDfobKMiEgVUriLiFQhhbuISBWysM46m1k/kHuFnXNbCAz42JyZQH2uDepzbSilzxc556a9RD+0cC+Fme1yznWH3Y5yUp9rg/pcG8rRZ5VlRESqkMJdRKQKzdRw3xR2A0KgPtcG9bk2BN7nGVlzFxGR3GbqyF1ERHKYceFuZuvM7AUzO2RmG8Nuj1/M7EEzO2FmvRP2zTezX5nZi9623dtvZnaf9zPYY2ZXhdfy4pnZMjN7wsz2mdleM7vH21+1/TazRjP7rZk95/X5r739y83saa9v/2xmDd7+Od7zQ97rXWG2v1hmVmdmvzezX3jPq7q/AGZ2xMyeN7NnzWyXt69sv9szKtzNrA64H7gBWA3cbmarw22VbzYD6ybt2wg85pxbATzmPYds/1d4fzYAD5SpjX4bBf7cObcaWAvc5f19VnO/zwDvc869C7gSWGdma4FvAt9xzr0DGAI+5R3/KWDI2/8d77iZ6B5g/4Tn1d7fMe91zl05Ydpj+X63nXMz5g9wHbBjwvMvAV8Ku10+9q8L6J3w/AVgifd4CfCC9/gfgdunOm4m/wF+Dry/VvoNNAO/A64le0HLbG//+O85sAO4zns82zvOwm57gf3s9ILsfcAvAKvm/k7o9xFg4aR9ZfvdnlEjd2ApcHTC8z5vX7XqcM695j1+HejwHlfdz8H73+81wNNUeb+9EsWzwAngV8BhIOqcG/UOmdiv8T57r58EZtqd2f8e+Asg4z1fQHX3d4wDHjGz3Wa2wdtXtt9tLfk7QzjnnJlV5dQmM5sL/AT4vHPulJmNv1aN/XbOpYErzawN+CmwMuQmBcbMPgiccM7tNrOesNtTZn/onDtmZucDvzKzAxNfDPp3e6aN3I8ByyY87/T2Vas3zGwJgLc94e2vmp+DmdWTDfaHnHP/29td9f0GcM5FgSfIliXazGxssDWxX+N99l6fBwyWuamluB642cyOAFvIlmb+K9Xb33HOuWPe9gTZf8SvoYy/2zMt3J8BVnhn2huA24BtIbcpSNuAP/Ee/wnZmvTY/k96Z9jXAicn/K/ejGHZIfr/APY75/7LhJeqtt9mtsgbsWNmTWTPMewnG/If8w6b3Oexn8XHgMedV5SdCZxzX3LOdTrnusj+9/q4c+4TVGl/x5hZi5m1jj0GPgD0Us7f7bBPOhRxkuJG4CDZOuVfht0eH/v1I+A1IEW23vYpsrXGx4AXgUeB+d6xRnbW0GHgeaA77PYX2ec/JFuX3AM86/25sZr7DVwB/N7rcy/wFW//xcBvgUPAj4E53v5G7/kh7/WLw+5DCX3vAX5RC/31+vec92fvWFaV83dbV6iKiFShmVaWERGRPCjcRUSqkMJdRKQKKdxFRKqQwl1EpAop3EVEqpDCXUSkCincRUSq0P8HUZpv4KM4dO8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1fc001b790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(num_epochs), train_accuracy)\n",
    "plt.plot(range(num_epochs), valid_accuracy)\n",
    "plt.grid(True)\n",
    "plt.legend(('train', 'valid'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8786"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO next...\n",
    "* try learning on a random class vector. Should get bad validation performance...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_discriminator as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_loop_info = md.get_speaker_info_for_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = md.get_speaker_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = md.get_train_valid_split(embeddings, speaker_info_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, criterion, optimizer = md.build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized: train 0.600 / validation 0.333\n",
      "Epoch 0: loss 0.691281, train 0.600 / validation 0.333\n",
      "Epoch 1: loss 0.690143, train 0.600 / validation 0.333\n",
      "Epoch 2: loss 0.689019, train 0.600 / validation 0.333\n",
      "Epoch 3: loss 0.687899, train 0.600 / validation 0.333\n",
      "Epoch 4: loss 0.686791, train 0.600 / validation 0.333\n",
      "Epoch 5: loss 0.685687, train 0.600 / validation 0.333\n",
      "Epoch 6: loss 0.684586, train 0.600 / validation 0.333\n",
      "Epoch 7: loss 0.683491, train 0.600 / validation 0.333\n",
      "Epoch 8: loss 0.682390, train 0.600 / validation 0.333\n",
      "Epoch 9: loss 0.681278, train 0.600 / validation 0.333\n",
      "Epoch 10: loss 0.680182, train 0.600 / validation 0.333\n",
      "Epoch 11: loss 0.679096, train 0.600 / validation 0.333\n",
      "Epoch 12: loss 0.678014, train 0.600 / validation 0.333\n",
      "Epoch 13: loss 0.676944, train 0.600 / validation 0.333\n",
      "Epoch 14: loss 0.675831, train 0.600 / validation 0.333\n",
      "Epoch 15: loss 0.674656, train 0.600 / validation 0.333\n",
      "Epoch 16: loss 0.673420, train 0.600 / validation 0.333\n",
      "Epoch 17: loss 0.672129, train 0.600 / validation 0.333\n",
      "Epoch 18: loss 0.670787, train 0.600 / validation 0.333\n",
      "Epoch 19: loss 0.669400, train 0.600 / validation 0.333\n",
      "Epoch 20: loss 0.667963, train 0.600 / validation 0.333\n",
      "Epoch 21: loss 0.666473, train 0.600 / validation 0.333\n",
      "Epoch 22: loss 0.664933, train 0.600 / validation 0.333\n",
      "Epoch 23: loss 0.663338, train 0.600 / validation 0.333\n",
      "Epoch 24: loss 0.661686, train 0.600 / validation 0.333\n",
      "Epoch 25: loss 0.659973, train 0.600 / validation 0.333\n",
      "Epoch 26: loss 0.658195, train 0.600 / validation 0.333\n",
      "Epoch 27: loss 0.656355, train 0.600 / validation 0.333\n",
      "Epoch 28: loss 0.654436, train 0.600 / validation 0.333\n",
      "Epoch 29: loss 0.652427, train 0.600 / validation 0.333\n",
      "Epoch 30: loss 0.650346, train 0.600 / validation 0.333\n",
      "Epoch 31: loss 0.648224, train 0.600 / validation 0.333\n",
      "Epoch 32: loss 0.646050, train 0.600 / validation 0.333\n",
      "Epoch 33: loss 0.643825, train 0.600 / validation 0.333\n",
      "Epoch 34: loss 0.641516, train 0.600 / validation 0.333\n",
      "Epoch 35: loss 0.639154, train 0.600 / validation 0.333\n",
      "Epoch 36: loss 0.636731, train 0.600 / validation 0.333\n",
      "Epoch 37: loss 0.634237, train 0.600 / validation 0.333\n",
      "Epoch 38: loss 0.631639, train 0.600 / validation 0.333\n",
      "Epoch 39: loss 0.628971, train 0.600 / validation 0.333\n",
      "Epoch 40: loss 0.626211, train 0.611 / validation 0.333\n",
      "Epoch 41: loss 0.623357, train 0.611 / validation 0.333\n",
      "Epoch 42: loss 0.620415, train 0.611 / validation 0.333\n",
      "Epoch 43: loss 0.617403, train 0.621 / validation 0.333\n",
      "Epoch 44: loss 0.614317, train 0.653 / validation 0.333\n",
      "Epoch 45: loss 0.611150, train 0.705 / validation 0.333\n",
      "Epoch 46: loss 0.607899, train 0.716 / validation 0.333\n",
      "Epoch 47: loss 0.604553, train 0.747 / validation 0.333\n",
      "Epoch 48: loss 0.601120, train 0.800 / validation 0.333\n",
      "Epoch 49: loss 0.597593, train 0.811 / validation 0.333\n",
      "Epoch 50: loss 0.593961, train 0.863 / validation 0.417\n",
      "Epoch 51: loss 0.590211, train 0.895 / validation 0.417\n",
      "Epoch 52: loss 0.586340, train 0.926 / validation 0.500\n",
      "Epoch 53: loss 0.582337, train 0.958 / validation 0.500\n",
      "Epoch 54: loss 0.578210, train 0.958 / validation 0.500\n",
      "Epoch 55: loss 0.573946, train 0.958 / validation 0.500\n",
      "Epoch 56: loss 0.569566, train 0.979 / validation 0.583\n",
      "Epoch 57: loss 0.565057, train 0.979 / validation 0.583\n",
      "Epoch 58: loss 0.560408, train 0.979 / validation 0.667\n",
      "Epoch 59: loss 0.555633, train 0.979 / validation 0.833\n",
      "Epoch 60: loss 0.550728, train 1.000 / validation 0.833\n",
      "Epoch 61: loss 0.545691, train 1.000 / validation 0.833\n",
      "Epoch 62: loss 0.540523, train 1.000 / validation 0.833\n",
      "Epoch 63: loss 0.535213, train 1.000 / validation 0.833\n",
      "Epoch 64: loss 0.529771, train 1.000 / validation 0.833\n",
      "Epoch 65: loss 0.524199, train 1.000 / validation 0.833\n",
      "Epoch 66: loss 0.518498, train 1.000 / validation 0.917\n",
      "Epoch 67: loss 0.512675, train 1.000 / validation 0.917\n",
      "Epoch 68: loss 0.506761, train 1.000 / validation 0.917\n",
      "Epoch 69: loss 0.500749, train 1.000 / validation 0.917\n",
      "Epoch 70: loss 0.494624, train 1.000 / validation 0.917\n",
      "Epoch 71: loss 0.488388, train 1.000 / validation 0.917\n",
      "Epoch 72: loss 0.482058, train 1.000 / validation 0.917\n",
      "Epoch 73: loss 0.475656, train 1.000 / validation 0.917\n",
      "Epoch 74: loss 0.469173, train 1.000 / validation 0.917\n",
      "Epoch 75: loss 0.462604, train 1.000 / validation 0.917\n",
      "Epoch 76: loss 0.455939, train 1.000 / validation 0.917\n",
      "Epoch 77: loss 0.449187, train 1.000 / validation 0.917\n",
      "Epoch 78: loss 0.442346, train 1.000 / validation 1.000\n",
      "Epoch 79: loss 0.435412, train 1.000 / validation 1.000\n",
      "Epoch 80: loss 0.428393, train 1.000 / validation 1.000\n",
      "Epoch 81: loss 0.421301, train 1.000 / validation 1.000\n",
      "Epoch 82: loss 0.414144, train 1.000 / validation 1.000\n",
      "Epoch 83: loss 0.406922, train 1.000 / validation 1.000\n",
      "Epoch 84: loss 0.399633, train 1.000 / validation 1.000\n",
      "Epoch 85: loss 0.392278, train 1.000 / validation 1.000\n",
      "Epoch 86: loss 0.384870, train 1.000 / validation 1.000\n",
      "Epoch 87: loss 0.377417, train 1.000 / validation 1.000\n",
      "Epoch 88: loss 0.369917, train 1.000 / validation 1.000\n",
      "Epoch 89: loss 0.362376, train 1.000 / validation 1.000\n",
      "Epoch 90: loss 0.354805, train 1.000 / validation 1.000\n",
      "Epoch 91: loss 0.347205, train 1.000 / validation 1.000\n",
      "Epoch 92: loss 0.339587, train 1.000 / validation 1.000\n",
      "Epoch 93: loss 0.331956, train 1.000 / validation 1.000\n",
      "Epoch 94: loss 0.324321, train 1.000 / validation 1.000\n",
      "Epoch 95: loss 0.316695, train 1.000 / validation 1.000\n",
      "Epoch 96: loss 0.309107, train 1.000 / validation 1.000\n",
      "Epoch 97: loss 0.301548, train 1.000 / validation 1.000\n",
      "Epoch 98: loss 0.294018, train 1.000 / validation 1.000\n",
      "Epoch 99: loss 0.286518, train 1.000 / validation 1.000\n",
      "Epoch 100: loss 0.279060, train 1.000 / validation 1.000\n",
      "Epoch 101: loss 0.271654, train 1.000 / validation 1.000\n",
      "Epoch 102: loss 0.264308, train 1.000 / validation 1.000\n",
      "Epoch 103: loss 0.257034, train 1.000 / validation 1.000\n",
      "Epoch 104: loss 0.249840, train 1.000 / validation 1.000\n",
      "Epoch 105: loss 0.242733, train 1.000 / validation 1.000\n",
      "Epoch 106: loss 0.235722, train 1.000 / validation 1.000\n",
      "Epoch 107: loss 0.228812, train 1.000 / validation 1.000\n",
      "Epoch 108: loss 0.222009, train 1.000 / validation 1.000\n",
      "Epoch 109: loss 0.215317, train 1.000 / validation 1.000\n",
      "Epoch 110: loss 0.208741, train 1.000 / validation 1.000\n",
      "Epoch 111: loss 0.202287, train 1.000 / validation 1.000\n",
      "Epoch 112: loss 0.195961, train 1.000 / validation 1.000\n",
      "Epoch 113: loss 0.189768, train 1.000 / validation 1.000\n",
      "Epoch 114: loss 0.183711, train 1.000 / validation 1.000\n",
      "Epoch 115: loss 0.177790, train 1.000 / validation 1.000\n",
      "Epoch 116: loss 0.172011, train 1.000 / validation 1.000\n",
      "Epoch 117: loss 0.166376, train 1.000 / validation 1.000\n",
      "Epoch 118: loss 0.160888, train 1.000 / validation 1.000\n",
      "Epoch 119: loss 0.155547, train 1.000 / validation 1.000\n",
      "Epoch 120: loss 0.150354, train 1.000 / validation 1.000\n",
      "Epoch 121: loss 0.145309, train 1.000 / validation 1.000\n",
      "Epoch 122: loss 0.140412, train 1.000 / validation 1.000\n",
      "Epoch 123: loss 0.135664, train 1.000 / validation 1.000\n",
      "Epoch 124: loss 0.131060, train 1.000 / validation 1.000\n",
      "Epoch 125: loss 0.126598, train 1.000 / validation 1.000\n",
      "Epoch 126: loss 0.122266, train 1.000 / validation 1.000\n",
      "Epoch 127: loss 0.118053, train 1.000 / validation 1.000\n",
      "Epoch 128: loss 0.113945, train 1.000 / validation 1.000\n",
      "Epoch 129: loss 0.109931, train 1.000 / validation 1.000\n",
      "Epoch 130: loss 0.106001, train 1.000 / validation 1.000\n",
      "Epoch 131: loss 0.102154, train 1.000 / validation 1.000\n",
      "Epoch 132: loss 0.098389, train 1.000 / validation 1.000\n",
      "Epoch 133: loss 0.094715, train 1.000 / validation 1.000\n",
      "Epoch 134: loss 0.091129, train 1.000 / validation 1.000\n",
      "Epoch 135: loss 0.087654, train 1.000 / validation 1.000\n",
      "Epoch 136: loss 0.084314, train 1.000 / validation 1.000\n",
      "Epoch 137: loss 0.081117, train 1.000 / validation 1.000\n",
      "Epoch 138: loss 0.078036, train 1.000 / validation 1.000\n",
      "Epoch 139: loss 0.075070, train 1.000 / validation 1.000\n",
      "Epoch 140: loss 0.072218, train 1.000 / validation 1.000\n",
      "Epoch 141: loss 0.069479, train 1.000 / validation 1.000\n",
      "Epoch 142: loss 0.066851, train 1.000 / validation 1.000\n",
      "Epoch 143: loss 0.064335, train 1.000 / validation 1.000\n",
      "Epoch 144: loss 0.061939, train 1.000 / validation 1.000\n",
      "Epoch 145: loss 0.059698, train 1.000 / validation 1.000\n",
      "Epoch 146: loss 0.057579, train 1.000 / validation 1.000\n",
      "Epoch 147: loss 0.055568, train 1.000 / validation 1.000\n",
      "Epoch 148: loss 0.053637, train 1.000 / validation 1.000\n",
      "Epoch 149: loss 0.051778, train 1.000 / validation 1.000\n",
      "Epoch 150: loss 0.049991, train 1.000 / validation 1.000\n",
      "Epoch 151: loss 0.048274, train 1.000 / validation 1.000\n",
      "Epoch 152: loss 0.046627, train 1.000 / validation 1.000\n",
      "Epoch 153: loss 0.045048, train 1.000 / validation 1.000\n",
      "Epoch 154: loss 0.043541, train 1.000 / validation 1.000\n",
      "Epoch 155: loss 0.042103, train 1.000 / validation 1.000\n",
      "Epoch 156: loss 0.040731, train 1.000 / validation 1.000\n",
      "Epoch 157: loss 0.039421, train 1.000 / validation 1.000\n",
      "Epoch 158: loss 0.038172, train 1.000 / validation 1.000\n",
      "Epoch 159: loss 0.036979, train 1.000 / validation 1.000\n",
      "Epoch 160: loss 0.035841, train 1.000 / validation 1.000\n",
      "Epoch 161: loss 0.034754, train 1.000 / validation 1.000\n",
      "Epoch 162: loss 0.033715, train 1.000 / validation 1.000\n",
      "Epoch 163: loss 0.032721, train 1.000 / validation 1.000\n",
      "Epoch 164: loss 0.031768, train 1.000 / validation 1.000\n",
      "Epoch 165: loss 0.030855, train 1.000 / validation 1.000\n",
      "Epoch 166: loss 0.029977, train 1.000 / validation 1.000\n",
      "Epoch 167: loss 0.029135, train 1.000 / validation 1.000\n",
      "Epoch 168: loss 0.028327, train 1.000 / validation 1.000\n",
      "Epoch 169: loss 0.027551, train 1.000 / validation 1.000\n",
      "Epoch 170: loss 0.026807, train 1.000 / validation 1.000\n",
      "Epoch 171: loss 0.026094, train 1.000 / validation 1.000\n",
      "Epoch 172: loss 0.025409, train 1.000 / validation 1.000\n",
      "Epoch 173: loss 0.024751, train 1.000 / validation 1.000\n",
      "Epoch 174: loss 0.024120, train 1.000 / validation 1.000\n",
      "Epoch 175: loss 0.023513, train 1.000 / validation 1.000\n",
      "Epoch 176: loss 0.022930, train 1.000 / validation 1.000\n",
      "Epoch 177: loss 0.022369, train 1.000 / validation 1.000\n",
      "Epoch 178: loss 0.021829, train 1.000 / validation 1.000\n",
      "Epoch 179: loss 0.021310, train 1.000 / validation 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180: loss 0.020808, train 1.000 / validation 1.000\n",
      "Epoch 181: loss 0.020325, train 1.000 / validation 1.000\n",
      "Epoch 182: loss 0.019860, train 1.000 / validation 1.000\n",
      "Epoch 183: loss 0.019411, train 1.000 / validation 1.000\n",
      "Epoch 184: loss 0.018977, train 1.000 / validation 1.000\n",
      "Epoch 185: loss 0.018559, train 1.000 / validation 1.000\n",
      "Epoch 186: loss 0.018154, train 1.000 / validation 1.000\n",
      "Epoch 187: loss 0.017764, train 1.000 / validation 1.000\n",
      "Epoch 188: loss 0.017387, train 1.000 / validation 1.000\n",
      "Epoch 189: loss 0.017022, train 1.000 / validation 1.000\n",
      "Epoch 190: loss 0.016670, train 1.000 / validation 1.000\n",
      "Epoch 191: loss 0.016330, train 1.000 / validation 1.000\n",
      "Epoch 192: loss 0.016000, train 1.000 / validation 1.000\n",
      "Epoch 193: loss 0.015681, train 1.000 / validation 1.000\n",
      "Epoch 194: loss 0.015372, train 1.000 / validation 1.000\n",
      "Epoch 195: loss 0.015073, train 1.000 / validation 1.000\n",
      "Epoch 196: loss 0.014782, train 1.000 / validation 1.000\n",
      "Epoch 197: loss 0.014501, train 1.000 / validation 1.000\n",
      "Epoch 198: loss 0.014228, train 1.000 / validation 1.000\n",
      "Epoch 199: loss 0.013963, train 1.000 / validation 1.000\n",
      "Epoch 200: loss 0.013706, train 1.000 / validation 1.000\n",
      "Epoch 201: loss 0.013457, train 1.000 / validation 1.000\n",
      "Epoch 202: loss 0.013214, train 1.000 / validation 1.000\n",
      "Epoch 203: loss 0.012979, train 1.000 / validation 1.000\n",
      "Epoch 204: loss 0.012750, train 1.000 / validation 1.000\n",
      "Epoch 205: loss 0.012528, train 1.000 / validation 1.000\n",
      "Epoch 206: loss 0.012312, train 1.000 / validation 1.000\n",
      "Epoch 207: loss 0.012102, train 1.000 / validation 1.000\n",
      "Epoch 208: loss 0.011898, train 1.000 / validation 1.000\n",
      "Epoch 209: loss 0.011698, train 1.000 / validation 1.000\n",
      "Epoch 210: loss 0.011504, train 1.000 / validation 1.000\n",
      "Epoch 211: loss 0.011316, train 1.000 / validation 1.000\n",
      "Epoch 212: loss 0.011132, train 1.000 / validation 1.000\n",
      "Epoch 213: loss 0.010953, train 1.000 / validation 1.000\n",
      "Epoch 214: loss 0.010779, train 1.000 / validation 1.000\n",
      "Epoch 215: loss 0.010609, train 1.000 / validation 1.000\n",
      "Epoch 216: loss 0.010443, train 1.000 / validation 1.000\n",
      "Epoch 217: loss 0.010281, train 1.000 / validation 1.000\n",
      "Epoch 218: loss 0.010123, train 1.000 / validation 1.000\n",
      "Epoch 219: loss 0.009969, train 1.000 / validation 1.000\n",
      "Epoch 220: loss 0.009819, train 1.000 / validation 1.000\n",
      "Epoch 221: loss 0.009673, train 1.000 / validation 1.000\n",
      "Epoch 222: loss 0.009530, train 1.000 / validation 1.000\n",
      "Epoch 223: loss 0.009390, train 1.000 / validation 1.000\n",
      "Epoch 224: loss 0.009253, train 1.000 / validation 1.000\n",
      "Epoch 225: loss 0.009120, train 1.000 / validation 1.000\n",
      "Epoch 226: loss 0.008989, train 1.000 / validation 1.000\n",
      "Epoch 227: loss 0.008862, train 1.000 / validation 1.000\n",
      "Epoch 228: loss 0.008737, train 1.000 / validation 1.000\n",
      "Epoch 229: loss 0.008616, train 1.000 / validation 1.000\n",
      "Epoch 230: loss 0.008496, train 1.000 / validation 1.000\n",
      "Epoch 231: loss 0.008380, train 1.000 / validation 1.000\n",
      "Epoch 232: loss 0.008266, train 1.000 / validation 1.000\n",
      "Epoch 233: loss 0.008154, train 1.000 / validation 1.000\n",
      "Epoch 234: loss 0.008045, train 1.000 / validation 1.000\n",
      "Epoch 235: loss 0.007938, train 1.000 / validation 1.000\n",
      "Epoch 236: loss 0.007834, train 1.000 / validation 1.000\n",
      "Epoch 237: loss 0.007731, train 1.000 / validation 1.000\n",
      "Epoch 238: loss 0.007631, train 1.000 / validation 1.000\n",
      "Epoch 239: loss 0.007532, train 1.000 / validation 1.000\n",
      "Epoch 240: loss 0.007436, train 1.000 / validation 1.000\n",
      "Epoch 241: loss 0.007342, train 1.000 / validation 1.000\n",
      "Epoch 242: loss 0.007249, train 1.000 / validation 1.000\n",
      "Epoch 243: loss 0.007158, train 1.000 / validation 1.000\n",
      "Epoch 244: loss 0.007070, train 1.000 / validation 1.000\n",
      "Epoch 245: loss 0.006982, train 1.000 / validation 1.000\n",
      "Epoch 246: loss 0.006897, train 1.000 / validation 1.000\n",
      "Epoch 247: loss 0.006813, train 1.000 / validation 1.000\n",
      "Epoch 248: loss 0.006731, train 1.000 / validation 1.000\n",
      "Epoch 249: loss 0.006650, train 1.000 / validation 1.000\n",
      "Epoch 250: loss 0.006571, train 1.000 / validation 1.000\n",
      "Epoch 251: loss 0.006493, train 1.000 / validation 1.000\n",
      "Epoch 252: loss 0.006417, train 1.000 / validation 1.000\n",
      "Epoch 253: loss 0.006342, train 1.000 / validation 1.000\n",
      "Epoch 254: loss 0.006269, train 1.000 / validation 1.000\n",
      "Epoch 255: loss 0.006197, train 1.000 / validation 1.000\n",
      "Epoch 256: loss 0.006126, train 1.000 / validation 1.000\n",
      "Epoch 257: loss 0.006056, train 1.000 / validation 1.000\n",
      "Epoch 258: loss 0.005988, train 1.000 / validation 1.000\n",
      "Epoch 259: loss 0.005921, train 1.000 / validation 1.000\n",
      "Epoch 260: loss 0.005855, train 1.000 / validation 1.000\n",
      "Epoch 261: loss 0.005790, train 1.000 / validation 1.000\n",
      "Epoch 262: loss 0.005726, train 1.000 / validation 1.000\n",
      "Epoch 263: loss 0.005663, train 1.000 / validation 1.000\n",
      "Epoch 264: loss 0.005602, train 1.000 / validation 1.000\n",
      "Epoch 265: loss 0.005541, train 1.000 / validation 1.000\n",
      "Epoch 266: loss 0.005482, train 1.000 / validation 1.000\n",
      "Epoch 267: loss 0.005423, train 1.000 / validation 1.000\n",
      "Epoch 268: loss 0.005366, train 1.000 / validation 1.000\n",
      "Epoch 269: loss 0.005309, train 1.000 / validation 1.000\n",
      "Epoch 270: loss 0.005254, train 1.000 / validation 1.000\n",
      "Epoch 271: loss 0.005199, train 1.000 / validation 1.000\n",
      "Epoch 272: loss 0.005145, train 1.000 / validation 1.000\n",
      "Epoch 273: loss 0.005092, train 1.000 / validation 1.000\n",
      "Epoch 274: loss 0.005040, train 1.000 / validation 1.000\n",
      "Epoch 275: loss 0.004989, train 1.000 / validation 1.000\n",
      "Epoch 276: loss 0.004938, train 1.000 / validation 1.000\n",
      "Epoch 277: loss 0.004889, train 1.000 / validation 1.000\n",
      "Epoch 278: loss 0.004840, train 1.000 / validation 1.000\n",
      "Epoch 279: loss 0.004792, train 1.000 / validation 1.000\n",
      "Epoch 280: loss 0.004744, train 1.000 / validation 1.000\n",
      "Epoch 281: loss 0.004698, train 1.000 / validation 1.000\n",
      "Epoch 282: loss 0.004652, train 1.000 / validation 1.000\n",
      "Epoch 283: loss 0.004607, train 1.000 / validation 1.000\n",
      "Epoch 284: loss 0.004562, train 1.000 / validation 1.000\n",
      "Epoch 285: loss 0.004518, train 1.000 / validation 1.000\n",
      "Epoch 286: loss 0.004475, train 1.000 / validation 1.000\n",
      "Epoch 287: loss 0.004432, train 1.000 / validation 1.000\n",
      "Epoch 288: loss 0.004390, train 1.000 / validation 1.000\n",
      "Epoch 289: loss 0.004349, train 1.000 / validation 1.000\n",
      "Epoch 290: loss 0.004308, train 1.000 / validation 1.000\n",
      "Epoch 291: loss 0.004268, train 1.000 / validation 1.000\n",
      "Epoch 292: loss 0.004229, train 1.000 / validation 1.000\n",
      "Epoch 293: loss 0.004189, train 1.000 / validation 1.000\n",
      "Epoch 294: loss 0.004151, train 1.000 / validation 1.000\n",
      "Epoch 295: loss 0.004113, train 1.000 / validation 1.000\n",
      "Epoch 296: loss 0.004076, train 1.000 / validation 1.000\n",
      "Epoch 297: loss 0.004039, train 1.000 / validation 1.000\n",
      "Epoch 298: loss 0.004003, train 1.000 / validation 1.000\n",
      "Epoch 299: loss 0.003967, train 1.000 / validation 1.000\n",
      "Epoch 300: loss 0.003931, train 1.000 / validation 1.000\n",
      "Epoch 301: loss 0.003897, train 1.000 / validation 1.000\n",
      "Epoch 302: loss 0.003862, train 1.000 / validation 1.000\n",
      "Epoch 303: loss 0.003828, train 1.000 / validation 1.000\n",
      "Epoch 304: loss 0.003795, train 1.000 / validation 1.000\n",
      "Epoch 305: loss 0.003762, train 1.000 / validation 1.000\n",
      "Epoch 306: loss 0.003729, train 1.000 / validation 1.000\n",
      "Epoch 307: loss 0.003697, train 1.000 / validation 1.000\n",
      "Epoch 308: loss 0.003666, train 1.000 / validation 1.000\n",
      "Epoch 309: loss 0.003634, train 1.000 / validation 1.000\n",
      "Epoch 310: loss 0.003604, train 1.000 / validation 1.000\n",
      "Epoch 311: loss 0.003573, train 1.000 / validation 1.000\n",
      "Epoch 312: loss 0.003543, train 1.000 / validation 1.000\n",
      "Epoch 313: loss 0.003514, train 1.000 / validation 1.000\n",
      "Epoch 314: loss 0.003484, train 1.000 / validation 1.000\n",
      "Epoch 315: loss 0.003455, train 1.000 / validation 1.000\n",
      "Epoch 316: loss 0.003427, train 1.000 / validation 1.000\n",
      "Epoch 317: loss 0.003399, train 1.000 / validation 1.000\n",
      "Epoch 318: loss 0.003371, train 1.000 / validation 1.000\n",
      "Epoch 319: loss 0.003344, train 1.000 / validation 1.000\n",
      "Epoch 320: loss 0.003317, train 1.000 / validation 1.000\n",
      "Epoch 321: loss 0.003290, train 1.000 / validation 1.000\n",
      "Epoch 322: loss 0.003263, train 1.000 / validation 1.000\n",
      "Epoch 323: loss 0.003237, train 1.000 / validation 1.000\n",
      "Epoch 324: loss 0.003212, train 1.000 / validation 1.000\n",
      "Epoch 325: loss 0.003186, train 1.000 / validation 1.000\n",
      "Epoch 326: loss 0.003161, train 1.000 / validation 1.000\n",
      "Epoch 327: loss 0.003136, train 1.000 / validation 1.000\n",
      "Epoch 328: loss 0.003112, train 1.000 / validation 1.000\n",
      "Epoch 329: loss 0.003087, train 1.000 / validation 1.000\n",
      "Epoch 330: loss 0.003063, train 1.000 / validation 1.000\n",
      "Epoch 331: loss 0.003040, train 1.000 / validation 1.000\n",
      "Epoch 332: loss 0.003016, train 1.000 / validation 1.000\n",
      "Epoch 333: loss 0.002993, train 1.000 / validation 1.000\n",
      "Epoch 334: loss 0.002970, train 1.000 / validation 1.000\n",
      "Epoch 335: loss 0.002948, train 1.000 / validation 1.000\n",
      "Epoch 336: loss 0.002926, train 1.000 / validation 1.000\n",
      "Epoch 337: loss 0.002904, train 1.000 / validation 1.000\n",
      "Epoch 338: loss 0.002882, train 1.000 / validation 1.000\n",
      "Epoch 339: loss 0.002860, train 1.000 / validation 1.000\n",
      "Epoch 340: loss 0.002839, train 1.000 / validation 1.000\n",
      "Epoch 341: loss 0.002818, train 1.000 / validation 1.000\n",
      "Epoch 342: loss 0.002797, train 1.000 / validation 1.000\n",
      "Epoch 343: loss 0.002777, train 1.000 / validation 1.000\n",
      "Epoch 344: loss 0.002756, train 1.000 / validation 1.000\n",
      "Epoch 345: loss 0.002736, train 1.000 / validation 1.000\n",
      "Epoch 346: loss 0.002716, train 1.000 / validation 1.000\n",
      "Epoch 347: loss 0.002697, train 1.000 / validation 1.000\n",
      "Epoch 348: loss 0.002677, train 1.000 / validation 1.000\n",
      "Epoch 349: loss 0.002658, train 1.000 / validation 1.000\n",
      "Epoch 350: loss 0.002639, train 1.000 / validation 1.000\n",
      "Epoch 351: loss 0.002620, train 1.000 / validation 1.000\n",
      "Epoch 352: loss 0.002602, train 1.000 / validation 1.000\n",
      "Epoch 353: loss 0.002583, train 1.000 / validation 1.000\n",
      "Epoch 354: loss 0.002565, train 1.000 / validation 1.000\n",
      "Epoch 355: loss 0.002547, train 1.000 / validation 1.000\n",
      "Epoch 356: loss 0.002529, train 1.000 / validation 1.000\n",
      "Epoch 357: loss 0.002512, train 1.000 / validation 1.000\n",
      "Epoch 358: loss 0.002494, train 1.000 / validation 1.000\n",
      "Epoch 359: loss 0.002477, train 1.000 / validation 1.000\n",
      "Epoch 360: loss 0.002460, train 1.000 / validation 1.000\n",
      "Epoch 361: loss 0.002443, train 1.000 / validation 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362: loss 0.002426, train 1.000 / validation 1.000\n",
      "Epoch 363: loss 0.002410, train 1.000 / validation 1.000\n",
      "Epoch 364: loss 0.002394, train 1.000 / validation 1.000\n",
      "Epoch 365: loss 0.002377, train 1.000 / validation 1.000\n",
      "Epoch 366: loss 0.002361, train 1.000 / validation 1.000\n",
      "Epoch 367: loss 0.002346, train 1.000 / validation 1.000\n",
      "Epoch 368: loss 0.002330, train 1.000 / validation 1.000\n",
      "Epoch 369: loss 0.002314, train 1.000 / validation 1.000\n",
      "Epoch 370: loss 0.002299, train 1.000 / validation 1.000\n",
      "Epoch 371: loss 0.002284, train 1.000 / validation 1.000\n",
      "Epoch 372: loss 0.002269, train 1.000 / validation 1.000\n",
      "Epoch 373: loss 0.002254, train 1.000 / validation 1.000\n",
      "Epoch 374: loss 0.002239, train 1.000 / validation 1.000\n",
      "Epoch 375: loss 0.002225, train 1.000 / validation 1.000\n",
      "Epoch 376: loss 0.002210, train 1.000 / validation 1.000\n",
      "Epoch 377: loss 0.002196, train 1.000 / validation 1.000\n",
      "Epoch 378: loss 0.002182, train 1.000 / validation 1.000\n",
      "Epoch 379: loss 0.002168, train 1.000 / validation 1.000\n",
      "Epoch 380: loss 0.002154, train 1.000 / validation 1.000\n",
      "Epoch 381: loss 0.002140, train 1.000 / validation 1.000\n",
      "Epoch 382: loss 0.002126, train 1.000 / validation 1.000\n",
      "Epoch 383: loss 0.002113, train 1.000 / validation 1.000\n",
      "Epoch 384: loss 0.002099, train 1.000 / validation 1.000\n",
      "Epoch 385: loss 0.002086, train 1.000 / validation 1.000\n",
      "Epoch 386: loss 0.002073, train 1.000 / validation 1.000\n",
      "Epoch 387: loss 0.002060, train 1.000 / validation 1.000\n",
      "Epoch 388: loss 0.002047, train 1.000 / validation 1.000\n",
      "Epoch 389: loss 0.002035, train 1.000 / validation 1.000\n",
      "Epoch 390: loss 0.002022, train 1.000 / validation 1.000\n",
      "Epoch 391: loss 0.002010, train 1.000 / validation 1.000\n",
      "Epoch 392: loss 0.001997, train 1.000 / validation 1.000\n",
      "Epoch 393: loss 0.001985, train 1.000 / validation 1.000\n",
      "Epoch 394: loss 0.001973, train 1.000 / validation 1.000\n",
      "Epoch 395: loss 0.001961, train 1.000 / validation 1.000\n",
      "Epoch 396: loss 0.001949, train 1.000 / validation 1.000\n",
      "Epoch 397: loss 0.001937, train 1.000 / validation 1.000\n",
      "Epoch 398: loss 0.001925, train 1.000 / validation 1.000\n",
      "Epoch 399: loss 0.001914, train 1.000 / validation 1.000\n",
      "Epoch 400: loss 0.001902, train 1.000 / validation 1.000\n",
      "Epoch 401: loss 0.001891, train 1.000 / validation 1.000\n",
      "Epoch 402: loss 0.001880, train 1.000 / validation 1.000\n",
      "Epoch 403: loss 0.001869, train 1.000 / validation 1.000\n",
      "Epoch 404: loss 0.001857, train 1.000 / validation 1.000\n",
      "Epoch 405: loss 0.001847, train 1.000 / validation 1.000\n",
      "Epoch 406: loss 0.001836, train 1.000 / validation 1.000\n",
      "Epoch 407: loss 0.001825, train 1.000 / validation 1.000\n",
      "Epoch 408: loss 0.001814, train 1.000 / validation 1.000\n",
      "Epoch 409: loss 0.001804, train 1.000 / validation 1.000\n",
      "Epoch 410: loss 0.001793, train 1.000 / validation 1.000\n",
      "Epoch 411: loss 0.001783, train 1.000 / validation 1.000\n",
      "Epoch 412: loss 0.001772, train 1.000 / validation 1.000\n",
      "Epoch 413: loss 0.001762, train 1.000 / validation 1.000\n",
      "Epoch 414: loss 0.001752, train 1.000 / validation 1.000\n",
      "Epoch 415: loss 0.001742, train 1.000 / validation 1.000\n",
      "Epoch 416: loss 0.001732, train 1.000 / validation 1.000\n",
      "Epoch 417: loss 0.001722, train 1.000 / validation 1.000\n",
      "Epoch 418: loss 0.001713, train 1.000 / validation 1.000\n",
      "Epoch 419: loss 0.001703, train 1.000 / validation 1.000\n",
      "Epoch 420: loss 0.001693, train 1.000 / validation 1.000\n",
      "Epoch 421: loss 0.001684, train 1.000 / validation 1.000\n",
      "Epoch 422: loss 0.001674, train 1.000 / validation 1.000\n",
      "Epoch 423: loss 0.001665, train 1.000 / validation 1.000\n",
      "Epoch 424: loss 0.001656, train 1.000 / validation 1.000\n",
      "Epoch 425: loss 0.001647, train 1.000 / validation 1.000\n",
      "Epoch 426: loss 0.001637, train 1.000 / validation 1.000\n",
      "Epoch 427: loss 0.001628, train 1.000 / validation 1.000\n",
      "Epoch 428: loss 0.001619, train 1.000 / validation 1.000\n",
      "Epoch 429: loss 0.001611, train 1.000 / validation 1.000\n",
      "Epoch 430: loss 0.001602, train 1.000 / validation 1.000\n",
      "Epoch 431: loss 0.001593, train 1.000 / validation 1.000\n",
      "Epoch 432: loss 0.001584, train 1.000 / validation 1.000\n",
      "Epoch 433: loss 0.001576, train 1.000 / validation 1.000\n",
      "Epoch 434: loss 0.001567, train 1.000 / validation 1.000\n",
      "Epoch 435: loss 0.001559, train 1.000 / validation 1.000\n",
      "Epoch 436: loss 0.001550, train 1.000 / validation 1.000\n",
      "Epoch 437: loss 0.001542, train 1.000 / validation 1.000\n",
      "Epoch 438: loss 0.001534, train 1.000 / validation 1.000\n",
      "Epoch 439: loss 0.001526, train 1.000 / validation 1.000\n",
      "Epoch 440: loss 0.001517, train 1.000 / validation 1.000\n",
      "Epoch 441: loss 0.001509, train 1.000 / validation 1.000\n",
      "Epoch 442: loss 0.001501, train 1.000 / validation 1.000\n",
      "Epoch 443: loss 0.001494, train 1.000 / validation 1.000\n",
      "Epoch 444: loss 0.001486, train 1.000 / validation 1.000\n",
      "Epoch 445: loss 0.001478, train 1.000 / validation 1.000\n",
      "Epoch 446: loss 0.001470, train 1.000 / validation 1.000\n",
      "Epoch 447: loss 0.001462, train 1.000 / validation 1.000\n",
      "Epoch 448: loss 0.001455, train 1.000 / validation 1.000\n",
      "Epoch 449: loss 0.001447, train 1.000 / validation 1.000\n",
      "Epoch 450: loss 0.001440, train 1.000 / validation 1.000\n",
      "Epoch 451: loss 0.001432, train 1.000 / validation 1.000\n",
      "Epoch 452: loss 0.001425, train 1.000 / validation 1.000\n",
      "Epoch 453: loss 0.001418, train 1.000 / validation 1.000\n",
      "Epoch 454: loss 0.001410, train 1.000 / validation 1.000\n",
      "Epoch 455: loss 0.001403, train 1.000 / validation 1.000\n",
      "Epoch 456: loss 0.001396, train 1.000 / validation 1.000\n",
      "Epoch 457: loss 0.001389, train 1.000 / validation 1.000\n",
      "Epoch 458: loss 0.001382, train 1.000 / validation 1.000\n",
      "Epoch 459: loss 0.001375, train 1.000 / validation 1.000\n",
      "Epoch 460: loss 0.001368, train 1.000 / validation 1.000\n",
      "Epoch 461: loss 0.001361, train 1.000 / validation 1.000\n",
      "Epoch 462: loss 0.001354, train 1.000 / validation 1.000\n",
      "Epoch 463: loss 0.001348, train 1.000 / validation 1.000\n",
      "Epoch 464: loss 0.001341, train 1.000 / validation 1.000\n",
      "Epoch 465: loss 0.001334, train 1.000 / validation 1.000\n",
      "Epoch 466: loss 0.001328, train 1.000 / validation 1.000\n",
      "Epoch 467: loss 0.001321, train 1.000 / validation 1.000\n",
      "Epoch 468: loss 0.001314, train 1.000 / validation 1.000\n",
      "Epoch 469: loss 0.001308, train 1.000 / validation 1.000\n",
      "Epoch 470: loss 0.001302, train 1.000 / validation 1.000\n",
      "Epoch 471: loss 0.001295, train 1.000 / validation 1.000\n",
      "Epoch 472: loss 0.001289, train 1.000 / validation 1.000\n",
      "Epoch 473: loss 0.001283, train 1.000 / validation 1.000\n",
      "Epoch 474: loss 0.001276, train 1.000 / validation 1.000\n",
      "Epoch 475: loss 0.001270, train 1.000 / validation 1.000\n",
      "Epoch 476: loss 0.001264, train 1.000 / validation 1.000\n",
      "Epoch 477: loss 0.001258, train 1.000 / validation 1.000\n",
      "Epoch 478: loss 0.001252, train 1.000 / validation 1.000\n",
      "Epoch 479: loss 0.001246, train 1.000 / validation 1.000\n",
      "Epoch 480: loss 0.001240, train 1.000 / validation 1.000\n",
      "Epoch 481: loss 0.001234, train 1.000 / validation 1.000\n",
      "Epoch 482: loss 0.001228, train 1.000 / validation 1.000\n",
      "Epoch 483: loss 0.001222, train 1.000 / validation 1.000\n",
      "Epoch 484: loss 0.001217, train 1.000 / validation 1.000\n",
      "Epoch 485: loss 0.001211, train 1.000 / validation 1.000\n",
      "Epoch 486: loss 0.001205, train 1.000 / validation 1.000\n",
      "Epoch 487: loss 0.001199, train 1.000 / validation 1.000\n",
      "Epoch 488: loss 0.001194, train 1.000 / validation 1.000\n",
      "Epoch 489: loss 0.001188, train 1.000 / validation 1.000\n",
      "Epoch 490: loss 0.001183, train 1.000 / validation 1.000\n",
      "Epoch 491: loss 0.001177, train 1.000 / validation 1.000\n",
      "Epoch 492: loss 0.001172, train 1.000 / validation 1.000\n",
      "Epoch 493: loss 0.001166, train 1.000 / validation 1.000\n",
      "Epoch 494: loss 0.001161, train 1.000 / validation 1.000\n",
      "Epoch 495: loss 0.001155, train 1.000 / validation 1.000\n",
      "Epoch 496: loss 0.001150, train 1.000 / validation 1.000\n",
      "Epoch 497: loss 0.001145, train 1.000 / validation 1.000\n",
      "Epoch 498: loss 0.001140, train 1.000 / validation 1.000\n",
      "Epoch 499: loss 0.001134, train 1.000 / validation 1.000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "net, train_accuracy, valid_accuracy, train_loss, valid_loss = md.train_discriminator(net, train_data, valid_data, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(num_epochs), train_accuracy)\n",
    "plt.plot(range(num_epochs), valid_accuracy)\n",
    "plt.grid(True)\n",
    "plt.legend(('train', 'valid'))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(num_epochs), train_loss)\n",
    "plt.plot(range(num_epochs), valid_loss)\n",
    "plt.grid(True)\n",
    "plt.legend(('train', 'valid'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.LongTensor(10).random_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(r + torch.LongTensor(10).random_(2 - 1) + 1) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False,  True,  True,  True,  True, False, False, False, False,\n",
       "       False,  True,  True,  True, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True, False, False,  True,\n",
       "       False, False, False, False, False,  True,  True, False, False,\n",
       "        True,  True, False, False, False, False,  True,  True, False,\n",
       "       False, False, False,  True, False, False, False,  True, False,\n",
       "       False, False, False, False,  True, False, False, False,  True,\n",
       "        True, False, False,  True, False,  True,  True, False, False,\n",
       "       False,  True, False,  True,  True,  True,  True, False,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data[1] + 1) % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = speaker_info_loop[10:11].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>236</td>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Manchester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  age gender  accents      region\n",
       "107  236   25      F  English  Manchester"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.age = 25\n",
    "tmp.index.set_value(tmp.index, 10, 107)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.drop('index', axis = 1, inplace=True)\n",
    "df[\"Country\"].replace(\"Republic of Korea\", value=\"South Korea\", inplace=True)\n",
    "df.set_index(\"Country\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_value() takes exactly 4 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-2a2b79fbaa89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m107\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: set_value() takes exactly 4 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "tmp.index.set_value(107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>226</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Surrey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>227</td>\n",
       "      <td>38</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Cumbria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>230</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Stockton-on-tees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>232</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>233</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Staffordshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>234</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>West Dumfries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>236</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Manchester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>237</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>Fife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>238</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>NorthernIrish</td>\n",
       "      <td>Belfast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>239</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>SW England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>240</td>\n",
       "      <td>21</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Southern England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>241</td>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>243</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>244</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Manchester</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>245</td>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "      <td>Irish</td>\n",
       "      <td>Dublin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>246</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>Selkirk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>247</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>Argyll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>248</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>Indian</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>249</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>Aberdeen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>250</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>SE England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>251</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>Indian</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>252</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>Edinburgh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>253</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>Welsh</td>\n",
       "      <td>Cardiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>254</td>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Surrey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>255</td>\n",
       "      <td>19</td>\n",
       "      <td>M</td>\n",
       "      <td>Scottish</td>\n",
       "      <td>Galloway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>256</td>\n",
       "      <td>24</td>\n",
       "      <td>M</td>\n",
       "      <td>English</td>\n",
       "      <td>Birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>311</td>\n",
       "      <td>21</td>\n",
       "      <td>M</td>\n",
       "      <td>American</td>\n",
       "      <td>Iowa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>312</td>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Hamilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>313</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>Irish</td>\n",
       "      <td>County Down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>314</td>\n",
       "      <td>26</td>\n",
       "      <td>F</td>\n",
       "      <td>SouthAfrican</td>\n",
       "      <td>Cape Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>316</td>\n",
       "      <td>20</td>\n",
       "      <td>M</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Alberta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>317</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Hamilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>318</td>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td>Napa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>323</td>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>SouthAfrican</td>\n",
       "      <td>Pretoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>326</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>Australian</td>\n",
       "      <td>English Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>329</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>330</td>\n",
       "      <td>26</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>333</td>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td>Indiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>334</td>\n",
       "      <td>18</td>\n",
       "      <td>M</td>\n",
       "      <td>American</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>335</td>\n",
       "      <td>25</td>\n",
       "      <td>F</td>\n",
       "      <td>NewZealand</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>336</td>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>SouthAfrican</td>\n",
       "      <td>Johannesburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>339</td>\n",
       "      <td>21</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td>Pennsylvania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>340</td>\n",
       "      <td>18</td>\n",
       "      <td>F</td>\n",
       "      <td>Irish</td>\n",
       "      <td>Dublin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>341</td>\n",
       "      <td>26</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td>Ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>343</td>\n",
       "      <td>27</td>\n",
       "      <td>F</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Alberta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>345</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>American</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>347</td>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>SouthAfrican</td>\n",
       "      <td>Johannesburg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>351</td>\n",
       "      <td>21</td>\n",
       "      <td>F</td>\n",
       "      <td>NorthernIrish</td>\n",
       "      <td>Derry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>360</td>\n",
       "      <td>19</td>\n",
       "      <td>M</td>\n",
       "      <td>American</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>361</td>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td>New Jersey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>362</td>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>American</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>363</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>Canadian</td>\n",
       "      <td>Toronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>364</td>\n",
       "      <td>23</td>\n",
       "      <td>M</td>\n",
       "      <td>Irish</td>\n",
       "      <td>Donegal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>374</td>\n",
       "      <td>28</td>\n",
       "      <td>M</td>\n",
       "      <td>Australian</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>376</td>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>Indian</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>236</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>English</td>\n",
       "      <td>Manchester</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  age gender        accents            region\n",
       "0    225   23      F        English  Southern England\n",
       "1    226   22      M        English            Surrey\n",
       "2    227   38      M        English           Cumbria\n",
       "3    228   22      F        English  Southern England\n",
       "4    229   23      F        English  Southern England\n",
       "5    230   22      F        English  Stockton-on-tees\n",
       "6    231   23      F        English  Southern England\n",
       "7    232   23      M        English  Southern England\n",
       "8    233   23      F        English     Staffordshire\n",
       "9    234   22      F       Scottish     West Dumfries\n",
       "10   236   23      F        English        Manchester\n",
       "11   237   22      M       Scottish              Fife\n",
       "12   238   22      F  NorthernIrish           Belfast\n",
       "13   239   22      F        English        SW England\n",
       "14   240   21      F        English  Southern England\n",
       "15   241   21      M       Scottish             Perth\n",
       "16   243   22      M        English            London\n",
       "17   244   22      F        English        Manchester\n",
       "18   245   25      M          Irish            Dublin\n",
       "19   246   22      M       Scottish           Selkirk\n",
       "20   247   22      M       Scottish            Argyll\n",
       "21   248   23      F         Indian                  \n",
       "22   249   22      F       Scottish          Aberdeen\n",
       "23   250   22      F        English        SE England\n",
       "24   251   26      M         Indian                  \n",
       "25   252   22      M       Scottish         Edinburgh\n",
       "26   253   22      F          Welsh           Cardiff\n",
       "27   254   21      M        English            Surrey\n",
       "28   255   19      M       Scottish          Galloway\n",
       "29   256   24      M        English        Birmingham\n",
       "..   ...  ...    ...            ...               ...\n",
       "78   311   21      M       American              Iowa\n",
       "79   312   19      F       Canadian          Hamilton\n",
       "80   313   24      F          Irish       County Down\n",
       "81   314   26      F   SouthAfrican         Cape Town\n",
       "82   316   20      M       Canadian           Alberta\n",
       "83   317   23      F       Canadian          Hamilton\n",
       "84   318   32      F       American              Napa\n",
       "85   323   19      F   SouthAfrican          Pretoria\n",
       "86   326   26      M     Australian    English Sydney\n",
       "87   329   23      F       American                  \n",
       "88   330   26      F       American                  \n",
       "89   333   19      F       American           Indiana\n",
       "90   334   18      M       American           Chicago\n",
       "91   335   25      F     NewZealand           English\n",
       "92   336   18      F   SouthAfrican      Johannesburg\n",
       "93   339   21      F       American      Pennsylvania\n",
       "94   340   18      F          Irish            Dublin\n",
       "95   341   26      F       American              Ohio\n",
       "96   343   27      F       Canadian           Alberta\n",
       "97   345   22      M       American           Florida\n",
       "98   347   26      M   SouthAfrican      Johannesburg\n",
       "99   351   21      F  NorthernIrish             Derry\n",
       "100  360   19      M       American        New Jersey\n",
       "101  361   19      F       American        New Jersey\n",
       "102  362   29      F       American                  \n",
       "103  363   22      M       Canadian           Toronto\n",
       "104  364   23      M          Irish           Donegal\n",
       "105  374   28      M     Australian           English\n",
       "106  376   22      M         Indian                  \n",
       "107  236   23      F        English        Manchester\n",
       "\n",
       "[108 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([speaker_info_loop, tmp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.index.set_value(tmp.index, 10, 107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(10) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.stack([t, 1-t], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0  0  0  0\n",
       "[torch.FloatTensor of size 1x4]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = Variable(torch.FloatTensor([0,0,0,1])).view(1, -1)\n",
    "target = Variable(torch.LongTensor([3]))\n",
    "output*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-6.9078e+10\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "loss = criterion(output, output*1e10)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at saved speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_embeddings = np.load('start_embeddings.npy')\n",
    "#saved_embeddings = np.load('train_embeddings_random.npy')\n",
    "#saved_embeddings = np.load('train_embeddings_min.npy')\n",
    "saved_embeddings = np.load('train_embeddings_last.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = saved_embeddings[0]\n",
    "speaker_info = saved_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.632\n"
     ]
    }
   ],
   "source": [
    "print \"accuracy: %.3f\" % saved_embeddings[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ba1469750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot norm of speaker embeddings\n",
    "plt.barh(range(embeddings.shape[0]), np.sum(embeddings**2,1))\n",
    "plt.grid(True)\n",
    "plt.xlabel('L2 Norm')\n",
    "plt.ylabel('Speaker ID')\n",
    "plt.title('Norm of Speaker Embedding Vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if embeddings.shape[0] == 108:\n",
    "    embeddings = np.delete(embeddings, -1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_fit = pca.fit_transform(embeddings)\n",
    "\n",
    "# t-sne\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_fit = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03087769,  0.02831514], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_m = speaker_info['gender'] == 'M'\n",
    "idx_f = speaker_info['gender'] == 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ba00c4cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "m_plot = plt.scatter(pca_fit[idx_m, 0], pca_fit[idx_m, 1], c='b', label='male')\n",
    "f_plot = plt.scatter(pca_fit[idx_f, 0], pca_fit[idx_f, 1], c='r', label='female')\n",
    "leg = plt.legend(handles=[m_plot, f_plot])\n",
    "plt.grid(True)\n",
    "plt.title('PCA: Speakers by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ba0005d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "m_plot = plt.scatter(tsne_fit[idx_m, 0], tsne_fit[idx_m, 1], c='b', label='male')\n",
    "f_plot = plt.scatter(tsne_fit[idx_f, 0], tsne_fit[idx_f, 1], c='r', label='female')\n",
    "leg = plt.legend(handles=[m_plot, f_plot])\n",
    "plt.grid(True)\n",
    "plt.title('T-SNE: Speakers by Gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = '/home/ubuntu/msc-project-fader-networks/checkpoints/discriminator_test/minmodel.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "p311_111_0\n",
      "/home/ubuntu/loop/data/vctk-16khz-cmu-no-boundaries-all/norm_info/norm.dat\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'echo 1 1 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4  | /tools/SPTK-3.9/x2x +af > /home/ubuntu/msc-project-fader-networks/weight' returned non-zero exit status 127",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d40cd9e6163f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                       \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                       \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                       npz_path = '/home/ubuntu/loop/data/vctk-16khz-cmu-no-boundaries-all/numpy_features')\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/msc-project-fader-networks/notebook_utils.py\u001b[0m in \u001b[0;36mgenerate_sample_with_loop\u001b[0;34m(npz, text, spkr_id, gender, checkpoint, output_dir, npz_path, output_file_override, ident_override)\u001b[0m\n\u001b[1;32m    513\u001b[0m                         \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m                         \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                         norm_path)\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;31m# generate .wav file from original features for reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/msc-project-fader-networks/utils.pyc\u001b[0m in \u001b[0;36mgenerate_merlin_wav\u001b[0;34m(data, gen_dir, file_basename, norm_info_file, do_post_filtering, mgc_dim, fl, sr)\u001b[0m\n\u001b[1;32m    255\u001b[0m             .format(\n\u001b[1;32m    256\u001b[0m                 \u001b[0mline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msptk_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X2X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 weight=os.path.join(gen_dir, 'weight')), shell=True)\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         pe(\n",
      "\u001b[0;32m/home/ubuntu/msc-project-fader-networks/utils.pyc\u001b[0m in \u001b[0;36mpe\u001b[0;34m(cmd, shell)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mPrint\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexecute\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0mon\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/msc-project-fader-networks/utils.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(cmd, shell)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mreturn_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'echo 1 1 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4 1.4  | /tools/SPTK-3.9/x2x +af > /home/ubuntu/msc-project-fader-networks/weight' returned non-zero exit status 127"
     ]
    }
   ],
   "source": [
    "# generate the sample\n",
    "spkr_id = 0\n",
    "#text = 'The threats would be a major escalation of the dispute and sparked further falls on stock markets.'\n",
    "text = 'China responded by accusing the US of blackmail'\n",
    "text = 'How trying to stay cool could make the world even hotter'\n",
    "#text = 'he picked his tooth up off the floor'\n",
    "npz='p311_111.npz'\n",
    "text = ''\n",
    "loop_dict = nu.generate_sample_with_loop(spkr_id=spkr_id, \n",
    "                                      npz=npz, \n",
    "                                      text=text, \n",
    "                                      checkpoint=checkpoint_file, \n",
    "                                      output_dir ='./',\n",
    "                                      npz_path = '/home/ubuntu/loop/data/vctk-16khz-cmu-no-boundaries-all/numpy_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listen to the original (if based on a known sample)\n",
    "if npz:\n",
    "    IPython.display.display(Audio(loop_dict['output_orig_fname'] + '.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,SG93X3RyeWluZ190b19zdGF5X2Nvb2xfY291bGRfbWFrZV90aGVfd29ybGRfZXZlbl9ob3R0ZXIud2F2\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# listen to sample synthesized from VoiceLoop features\n",
    "Audio(loop_dict['output_file'] + '.wav', autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How_trying_to_stay_cool_could_make_the_world_even_hotter'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_dict['output_file'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
